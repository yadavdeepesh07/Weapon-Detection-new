{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0af588c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations saved to C:\\Users\\dsy92\\OneDrive\\Desktop\\Weapon-Detection\\enhance_image\\rcnn_data\\annotations.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "# Define paths\n",
    "image_folder = r'C:\\Users\\dsy92\\OneDrive\\Desktop\\Weapon-Detection\\enhance_image\\rcnn_data\\images_coco'\n",
    "json_output_path = r'C:\\Users\\dsy92\\OneDrive\\Desktop\\Weapon-Detection\\enhance_image\\rcnn_data\\annotations.json'\n",
    "\n",
    "# Function to convert images into COCO format annotations\n",
    "def create_coco_format(image_folder):\n",
    "    images = []\n",
    "    annotations = []\n",
    "    categories = [{'id': 1, 'name': 'weapon', 'supercategory': 'weapon'}]  # Adjust category as needed\n",
    "\n",
    "    image_id = 1\n",
    "    annotation_id = 1\n",
    "\n",
    "    # Loop through each image in the folder\n",
    "    for img_file in glob(os.path.join(image_folder, '*.jpg')):  # Assuming images are in .jpg format\n",
    "        img = Image.open(img_file)\n",
    "        width, height = img.size\n",
    "\n",
    "        # Image entry\n",
    "        image_entry = {\n",
    "            'id': image_id,\n",
    "            'file_name': os.path.basename(img_file),\n",
    "            'width': width,\n",
    "            'height': height\n",
    "        }\n",
    "        images.append(image_entry)\n",
    "\n",
    "        # Example: Add annotations (bounding box, etc.)\n",
    "        # You can adjust the bounding box and category information as per your requirement\n",
    "        annotation_entry = {\n",
    "            'id': annotation_id,\n",
    "            'image_id': image_id,\n",
    "            'category_id': 1,  # ID for 'weapon'\n",
    "            'bbox': [50, 50, 100, 100],  # Example bounding box, you need to adjust it based on your dataset\n",
    "            'area': 100 * 100,  # Area of the bounding box (width * height)\n",
    "            'iscrowd': 0\n",
    "        }\n",
    "        annotations.append(annotation_entry)\n",
    "\n",
    "        # Increment IDs for next image and annotation\n",
    "        image_id += 1\n",
    "        annotation_id += 1\n",
    "\n",
    "    # Combine everything into the final COCO format\n",
    "    coco_format = {\n",
    "        'images': images,\n",
    "        'annotations': annotations,\n",
    "        'categories': categories\n",
    "    }\n",
    "\n",
    "    return coco_format\n",
    "\n",
    "# Create COCO format annotations\n",
    "coco_data = create_coco_format(image_folder)\n",
    "\n",
    "# Save the data to a JSON file\n",
    "with open(json_output_path, 'w') as json_file:\n",
    "    json.dump(coco_data, json_file, indent=4)\n",
    "\n",
    "print(f\"Annotations saved to {json_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "233f0a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\compiler\\python\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\compiler\\python\\lib\\site-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: filelock in c:\\compiler\\python\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\compiler\\python\\lib\\site-packages (from torch) (4.13.0)\n",
      "Requirement already satisfied: networkx in c:\\compiler\\python\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\compiler\\python\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\compiler\\python\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\compiler\\python\\lib\\site-packages (from torch) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\compiler\\python\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\compiler\\python\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\compiler\\python\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\compiler\\python\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\compiler\\python\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faee92a",
   "metadata": {},
   "source": [
    "faster rcnn model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebb1e3a",
   "metadata": {},
   "source": [
    "data set prepartion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c45f05a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\compiler\\python\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: pillow in c:\\compiler\\python\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\compiler\\python\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5259138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36963/36963 [06:36<00:00, 93.16it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO annotations saved to C:\\Users\\dsy92\\OneDrive\\Desktop\\Weapon-Detection\\enhance_image\\rcnn_data\\annotations.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define paths\n",
    "image_dir = r'C:\\Users\\dsy92\\OneDrive\\Desktop\\Weapon-Detection\\enhance_image\\train\\images'\n",
    "label_dir = r'C:\\Users\\dsy92\\OneDrive\\Desktop\\Weapon-Detection\\enhance_image\\train\\labels'\n",
    "output_dir = r'C:\\Users\\dsy92\\OneDrive\\Desktop\\Weapon-Detection\\enhance_image\\rcnn_data'\n",
    "\n",
    "# Create output directories\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "# Initialize COCO format\n",
    "coco = {\n",
    "    \"images\": [],\n",
    "    \"annotations\": [],\n",
    "    \"categories\": [{\"id\": 1, \"name\": \"weapon\", \"supercategory\": \"none\"}]\n",
    "}\n",
    "\n",
    "annotation_id = 1\n",
    "image_id = 1\n",
    "\n",
    "# Iterate through all image files\n",
    "for image_name in tqdm(os.listdir(image_dir)):\n",
    "    if image_name.endswith(('.jpg', '.png')):  # Filter only image files\n",
    "        # Image path\n",
    "        image_path = os.path.join(image_dir, image_name)\n",
    "        \n",
    "        # Read image with OpenCV\n",
    "        image = cv2.imread(image_path)\n",
    "        height, width, _ = image.shape\n",
    "\n",
    "        # Image information for COCO format\n",
    "        image_info = {\n",
    "            \"id\": image_id,\n",
    "            \"width\": width,\n",
    "            \"height\": height,\n",
    "            \"file_name\": image_name\n",
    "        }\n",
    "\n",
    "        # Add image info to COCO\n",
    "        coco['images'].append(image_info)\n",
    "\n",
    "        # Label file corresponding to the image\n",
    "        label_file = os.path.join(label_dir, image_name.replace('.jpg', '.txt').replace('.png', '.txt'))\n",
    "        \n",
    "        if os.path.exists(label_file):\n",
    "            with open(label_file, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    # YOLO annotation: class x_center y_center width height (normalized)\n",
    "                    parts = line.strip().split()\n",
    "                    class_id = int(parts[0])\n",
    "                    x_center, y_center, w, h = map(float, parts[1:])\n",
    "                    \n",
    "                    # Convert YOLO to COCO format (bounding box is [x_min, y_min, width, height])\n",
    "                    x_min = int((x_center - w / 2) * width)\n",
    "                    y_min = int((y_center - h / 2) * height)\n",
    "                    bbox_width = int(w * width)\n",
    "                    bbox_height = int(h * height)\n",
    "\n",
    "                    # Annotation info for COCO format\n",
    "                    annotation = {\n",
    "                        \"id\": annotation_id,\n",
    "                        \"image_id\": image_id,\n",
    "                        \"category_id\": 1,  # category 'weapon'\n",
    "                        \"bbox\": [x_min, y_min, bbox_width, bbox_height],\n",
    "                        \"area\": bbox_width * bbox_height,\n",
    "                        \"iscrowd\": 0\n",
    "                    }\n",
    "                    \n",
    "                    # Add annotation info to COCO\n",
    "                    coco['annotations'].append(annotation)\n",
    "                    annotation_id += 1\n",
    "\n",
    "        # Increment image id\n",
    "        image_id += 1\n",
    "\n",
    "# Save COCO annotations to JSON file\n",
    "output_json_path = os.path.join(output_dir, 'annotations.json')\n",
    "with open(output_json_path, 'w') as json_file:\n",
    "    json.dump(coco, json_file)\n",
    "\n",
    "print(f\"COCO annotations saved to {output_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2336d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\compiler\\python\\lib\\site-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: pycocotools in c:\\compiler\\python\\lib\\site-packages (2.0.8)\n",
      "Requirement already satisfied: matplotlib in c:\\compiler\\python\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: tqdm in c:\\compiler\\python\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy in c:\\compiler\\python\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.5.1+cu121 in c:\\compiler\\python\\lib\\site-packages (from torchvision) (2.5.1+cu121)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\compiler\\python\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: filelock in c:\\compiler\\python\\lib\\site-packages (from torch==2.5.1+cu121->torchvision) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\compiler\\python\\lib\\site-packages (from torch==2.5.1+cu121->torchvision) (4.13.0)\n",
      "Requirement already satisfied: networkx in c:\\compiler\\python\\lib\\site-packages (from torch==2.5.1+cu121->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\compiler\\python\\lib\\site-packages (from torch==2.5.1+cu121->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\compiler\\python\\lib\\site-packages (from torch==2.5.1+cu121->torchvision) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\compiler\\python\\lib\\site-packages (from torch==2.5.1+cu121->torchvision) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\compiler\\python\\lib\\site-packages (from torch==2.5.1+cu121->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\compiler\\python\\lib\\site-packages (from sympy==1.13.1->torch==2.5.1+cu121->torchvision) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\compiler\\python\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\compiler\\python\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\compiler\\python\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\compiler\\python\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dsy92\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\compiler\\python\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dsy92\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dsy92\\appdata\\roaming\\python\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dsy92\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\compiler\\python\\lib\\site-packages (from jinja2->torch==2.5.1+cu121->torchvision) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision pycocotools matplotlib tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d48eba9",
   "metadata": {},
   "source": [
    "Dataset Class for COCO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "977b13de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision import transforms as T\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "\n",
    "class CocoWeaponDataset(CocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, transforms=None):\n",
    "        super(CocoWeaponDataset, self).__init__(img_folder, ann_file)\n",
    "        self.transforms = transforms\n",
    "        self.coco = COCO(ann_file)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = super(CocoWeaponDataset, self).__getitem__(idx)\n",
    "        image_id = self.ids[idx]\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann['category_id'])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([image_id])\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31605e7d",
   "metadata": {},
   "source": [
    "Dataset and DataLoader Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f452f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = r\"C:\\Users\\dsy92\\OneDrive\\Desktop\\Weapon-Detection\\enhance_image\\rcnn_data\\images_coco\"\n",
    "annotation_file = r\"C:\\Users\\dsy92\\OneDrive\\Desktop\\Weapon-Detection\\enhance_image\\rcnn_data\\annotations.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c991966c",
   "metadata": {},
   "source": [
    "Create dataset & loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f60c47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.08s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.08s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "dataset = CocoWeaponDataset(img_folder=image_dir, ann_file=annotation_file, transforms=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902f349a",
   "metadata": {},
   "source": [
    "Load and Modify Faster R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d0c21c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Set the number of classes (+1 for background)\n",
    "num_classes = 2  # weapon + background\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torch.nn.Linear(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9da4fd7",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40b5dc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_folder, annotation_file, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.annotation_file = annotation_file\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load annotations from the JSON file\n",
    "        with open(self.annotation_file, 'r') as f:\n",
    "            self.annotations = json.load(f)\n",
    "        \n",
    "        # Get all image paths\n",
    "        self.image_paths = [os.path.join(self.image_folder, f) for f in os.listdir(self.image_folder) if f.endswith('.jpg')]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image path\n",
    "        img_path = self.image_paths[idx]\n",
    "        \n",
    "        # Open the image\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Get the image name (ensure correct filename without path)\n",
    "        img_name = os.path.basename(img_path)\n",
    "        \n",
    "        # Check if the image name is in the annotations\n",
    "        target = self.annotations.get(img_name, None)\n",
    "        \n",
    "        # If no annotations are found for this image, return None or handle as you like\n",
    "        if target is None:\n",
    "            print(f\"Warning: No annotations found for {img_name}. Skipping...\")\n",
    "            return None, None  # You can choose to skip this image or return a default\n",
    "        \n",
    "        # Get the bounding boxes and labels (ensure that they are in the correct format)\n",
    "        boxes = torch.tensor(target['boxes'], dtype=torch.float32)\n",
    "        labels = torch.tensor(target['labels'], dtype=torch.int64)\n",
    "\n",
    "        # Create the target dictionary for Faster R-CNN\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc306b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Compiler\\Python\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Compiler\\Python\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have already defined CustomDataset class above\n",
    "# Define transforms for your data (e.g., normalization, resizing)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((800, 800)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Initialize the dataset and dataloaders\n",
    "train_dataset = CustomDataset(image_folder=r\"C:\\Users\\dsy92\\OneDrive\\Desktop\\Weapon-Detection\\enhance_image\\rcnn_data\\images_coco\", annotation_file=r\"C:\\Users\\dsy92\\OneDrive\\Desktop\\Weapon-Detection\\enhance_image\\rcnn_data\\annotations.json\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Modify the classifier to match the number of classes in your dataset (e.g., 2 classes: 'background' and 'weapon')\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes=2)  # For 2 classes\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a28c011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and scheduler\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Optionally, set up a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "04a7ca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, lr_scheduler=None):\n",
    "    model.train()\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    header = f\"Epoch #{epoch}\"\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, 10, header):\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        # Total loss\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update learning rate\n",
    "        if lr_scheduler:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        # Log the losses\n",
    "        metric_logger.update(loss=losses.item())\n",
    "    return metric_logger.meters['loss'].global_avg\n",
    "\n",
    "class MetricLogger:\n",
    "    # Helper class to log metrics like loss\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = {}\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for name, value in kwargs.items():\n",
    "            if name not in self.meters:\n",
    "                self.meters[name] = AverageMeter()\n",
    "            self.meters[name].update(value)\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        # Helper function to print logs during training\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        for i, obj in enumerate(iterable):\n",
    "            if i % print_freq == 0:\n",
    "                eta_seconds = (time.time() - start_time) * (len(iterable) - i)\n",
    "                eta = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                print(f\"{header} [{i}/{len(iterable)}]  {self.delimiter}  {self.meters}\")\n",
    "            yield obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c77aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train_one_epoch(model, optimizer, train_loader, device, epoch, lr_scheduler)\n",
    "    print(f\"Epoch {epoch} - Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4953e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.16.1 in c:\\compiler\\python\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\compiler\\python\\lib\\site-packages (from tensorflow==2.16.1) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\compiler\\python\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\compiler\\python\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\compiler\\python\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\compiler\\python\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\compiler\\python\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\compiler\\python\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (3.13.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\compiler\\python\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\compiler\\python\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (0.3.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\compiler\\python\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\dsy92\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\compiler\\python\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (4.25.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\compiler\\python\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\compiler\\python\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (78.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\dsy92\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\compiler\\python\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\compiler\\python\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (4.13.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\compiler\\python\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\compiler\\python\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (1.71.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\compiler\\python\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\compiler\\python\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\compiler\\python\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow==2.16.1) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\compiler\\python\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow==2.16.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\compiler\\python\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow==2.16.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\compiler\\python\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow==2.16.1) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\compiler\\python\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow==2.16.1) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\compiler\\python\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow==2.16.1) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\compiler\\python\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow==2.16.1) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\compiler\\python\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow==2.16.1) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\compiler\\python\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow==2.16.1) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\compiler\\python\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow==2.16.1) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\compiler\\python\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow==2.16.1) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\compiler\\python\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow==2.16.1) (0.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\compiler\\python\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow==2.16.1) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\compiler\\python\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow==2.16.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dsy92\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow==2.16.1) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\compiler\\python\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow==2.16.1) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==2.16.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8565e3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in c:\\compiler\\python\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\compiler\\python\\lib\\site-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: torchaudio in c:\\compiler\\python\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in c:\\compiler\\python\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\compiler\\python\\lib\\site-packages (from torch) (4.13.0)\n",
      "Requirement already satisfied: networkx in c:\\compiler\\python\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\compiler\\python\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\compiler\\python\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\compiler\\python\\lib\\site-packages (from torch) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\compiler\\python\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\compiler\\python\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\compiler\\python\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\compiler\\python\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\compiler\\python\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1a7401e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch GPU Support: True\n",
      "GPU Name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch GPU Support:\", torch.cuda.is_available())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d3eb0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "GPU Name: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "CUDA Version: 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\n",
    "\n",
    "# If available, get GPU details\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA Version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a9e02e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor on: cuda:0\n",
      "Model on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Example: Move a tensor to GPU\n",
    "x = torch.randn(3, 3).to(device)\n",
    "print(\"Tensor on:\", x.device)\n",
    "\n",
    "# Example: Move a model to GPU\n",
    "model = torch.nn.Linear(10, 5).to(device)\n",
    "print(\"Model on:\", next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e690707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dsy92\\AppData\\Local\\Temp\\ipykernel_19932\\2463452659.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "# Save model (ensure it's on CPU for compatibility)\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "# Load model (map to GPU if available)\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece2ec46",
   "metadata": {},
   "source": [
    "try on gpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc53f615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model to GPU\n",
    "model = MyModel(input_size=100, hidden_size=128, output_size=10).to(device)\n",
    "\n",
    "# Example DataLoader setup (ensure data is moved to GPU during training)\n",
    "train_loader = DataLoader(...)  # Replace with your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2404b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, train_loader, device, epoch, lr_scheduler):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Move data to GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    lr_scheduler.step()\n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8835c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# 1. Define a Dataset (replace with your data)\n",
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, input_size=100, output_size=10):\n",
    "        self.x = torch.randn(num_samples, input_size)  # Dummy input data\n",
    "        self.y = torch.randint(0, output_size, (num_samples,))  # Dummy labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "# 2. Initialize the Dataset and DataLoader\n",
    "train_dataset = DummyDataset(num_samples=1000)  # ✅ Replace `...` with your dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 3. Rest of the code (model, training loop, etc.) remains the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fcde34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Compiler\\Python\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Compiler\\Python\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Loss: 0.0023\n",
      "Epoch [2/10] Loss: 0.0005\n",
      "Epoch [3/10] Loss: 0.0003\n",
      "Epoch [4/10] Loss: 0.0002\n",
      "Epoch [5/10] Loss: 0.0003\n",
      "Epoch [6/10] Loss: 0.0016\n",
      "Epoch [7/10] Loss: 0.0003\n",
      "Epoch [8/10] Loss: 0.0006\n",
      "Epoch [9/10] Loss: 0.0004\n",
      "Epoch [10/10] Loss: 0.0002\n",
      "Training complete! Model saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# Custom Dataset Class\n",
    "class WeaponDetectionDataset(Dataset):\n",
    "    def __init__(self, img_dir, annotation_path, transforms=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # Load annotations\n",
    "        with open(annotation_path) as f:\n",
    "            self.coco_data = json.load(f)\n",
    "        \n",
    "        # Create image ID to annotations mapping\n",
    "        self.annot_dict = {}\n",
    "        for annot in self.coco_data['annotations']:\n",
    "            img_id = annot['image_id']\n",
    "            if img_id not in self.annot_dict:\n",
    "                self.annot_dict[img_id] = []\n",
    "            self.annot_dict[img_id].append(annot)\n",
    "        \n",
    "        # Get image metadata\n",
    "        self.images = self.coco_data['images']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_info['file_name'])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Get annotations for this image\n",
    "        annots = self.annot_dict.get(img_info['id'], [])\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for annot in annots:\n",
    "            # Convert COCO bbox format [x,y,width,height] to [x0,y0,x1,y1]\n",
    "            x, y, w, h = annot['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(annot['category_id'])\n",
    "        \n",
    "        # Convert to tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "# Transformations\n",
    "def get_transform():\n",
    "    return T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Resize((800, 800)),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                   std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "# Initialize Dataset and DataLoader\n",
    "img_dir = r\"C:\\Users\\dsy92\\OneDrive\\Desktop\\Weapon-Detection\\enhance_image\\rcnn_data\\images_coco\"\n",
    "annot_path = r\"C:\\Users\\dsy92\\OneDrive\\Desktop\\Weapon-Detection\\enhance_image\\rcnn_data\\annotations.json\"\n",
    "\n",
    "dataset = WeaponDetectionDataset(img_dir, annot_path, transforms=get_transform())\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch))\n",
    ")\n",
    "\n",
    "# Model setup\n",
    "def get_model(num_classes):\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "# Number of classes (including background)\n",
    "num_classes = 2  # Update with your actual number of classes + 1 (background)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = get_model(num_classes).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Training function\n",
    "def train_one_epoch(model, optimizer, data_loader, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for images, targets in data_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = train_one_epoch(model, optimizer, data_loader, device)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"weapon_detection_model.pth\")\n",
    "print(\"Training complete! Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15d2b760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "def calculate_iou_accuracy(pred_boxes, gt_boxes):\n",
    "    if len(pred_boxes) == 0 or len(gt_boxes) == 0:\n",
    "        return 0.0\n",
    "    ious = box_iou(pred_boxes, gt_boxes)\n",
    "    matches = (ious > 0.5).sum().item()\n",
    "    return matches / max(len(gt_boxes), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b04b537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_iou_acc = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for images, targets in data_loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += losses.item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            for output, target in zip(outputs, targets):\n",
    "                iou_acc = calculate_iou_accuracy(output['boxes'].cpu(), target['boxes'].cpu())\n",
    "                total_iou_acc += iou_acc\n",
    "                num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    avg_acc = total_iou_acc / num_batches\n",
    "\n",
    "    return avg_loss, avg_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0193cf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example loss/accuracy lists (replace with your actual lists)\n",
    "train_losses = [0.95, 0.76, 0.65, 0.58, 0.50, 0.43, 0.38, 0.33, 0.30, 0.27]\n",
    "train_accuracies = [0.45, 0.55, 0.60, 0.64, 0.68, 0.73, 0.76, 0.79, 0.82, 0.85]\n",
    "\n",
    "epochs = list(range(1, len(train_losses)+1))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label='Training Loss', color='blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, label='Training Accuracy', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('IoU Accuracy')\n",
    "plt.title('Accuracy per Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eec7418",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FasterRCNN:\n\tMissing key(s) in state_dict: \"backbone.body.conv1.weight\", \"backbone.body.bn1.weight\", \"backbone.body.bn1.bias\", \"backbone.body.bn1.running_mean\", \"backbone.body.bn1.running_var\", \"backbone.body.layer1.0.conv1.weight\", \"backbone.body.layer1.0.bn1.weight\", \"backbone.body.layer1.0.bn1.bias\", \"backbone.body.layer1.0.bn1.running_mean\", \"backbone.body.layer1.0.bn1.running_var\", \"backbone.body.layer1.0.conv2.weight\", \"backbone.body.layer1.0.bn2.weight\", \"backbone.body.layer1.0.bn2.bias\", \"backbone.body.layer1.0.bn2.running_mean\", \"backbone.body.layer1.0.bn2.running_var\", \"backbone.body.layer1.0.conv3.weight\", \"backbone.body.layer1.0.bn3.weight\", \"backbone.body.layer1.0.bn3.bias\", \"backbone.body.layer1.0.bn3.running_mean\", \"backbone.body.layer1.0.bn3.running_var\", \"backbone.body.layer1.0.downsample.0.weight\", \"backbone.body.layer1.0.downsample.1.weight\", \"backbone.body.layer1.0.downsample.1.bias\", \"backbone.body.layer1.0.downsample.1.running_mean\", \"backbone.body.layer1.0.downsample.1.running_var\", \"backbone.body.layer1.1.conv1.weight\", \"backbone.body.layer1.1.bn1.weight\", \"backbone.body.layer1.1.bn1.bias\", \"backbone.body.layer1.1.bn1.running_mean\", \"backbone.body.layer1.1.bn1.running_var\", \"backbone.body.layer1.1.conv2.weight\", \"backbone.body.layer1.1.bn2.weight\", \"backbone.body.layer1.1.bn2.bias\", \"backbone.body.layer1.1.bn2.running_mean\", \"backbone.body.layer1.1.bn2.running_var\", \"backbone.body.layer1.1.conv3.weight\", \"backbone.body.layer1.1.bn3.weight\", \"backbone.body.layer1.1.bn3.bias\", \"backbone.body.layer1.1.bn3.running_mean\", \"backbone.body.layer1.1.bn3.running_var\", \"backbone.body.layer1.2.conv1.weight\", \"backbone.body.layer1.2.bn1.weight\", \"backbone.body.layer1.2.bn1.bias\", \"backbone.body.layer1.2.bn1.running_mean\", \"backbone.body.layer1.2.bn1.running_var\", \"backbone.body.layer1.2.conv2.weight\", \"backbone.body.layer1.2.bn2.weight\", \"backbone.body.layer1.2.bn2.bias\", \"backbone.body.layer1.2.bn2.running_mean\", \"backbone.body.layer1.2.bn2.running_var\", \"backbone.body.layer1.2.conv3.weight\", \"backbone.body.layer1.2.bn3.weight\", \"backbone.body.layer1.2.bn3.bias\", \"backbone.body.layer1.2.bn3.running_mean\", \"backbone.body.layer1.2.bn3.running_var\", \"backbone.body.layer2.0.conv1.weight\", \"backbone.body.layer2.0.bn1.weight\", \"backbone.body.layer2.0.bn1.bias\", \"backbone.body.layer2.0.bn1.running_mean\", \"backbone.body.layer2.0.bn1.running_var\", \"backbone.body.layer2.0.conv2.weight\", \"backbone.body.layer2.0.bn2.weight\", \"backbone.body.layer2.0.bn2.bias\", \"backbone.body.layer2.0.bn2.running_mean\", \"backbone.body.layer2.0.bn2.running_var\", \"backbone.body.layer2.0.conv3.weight\", \"backbone.body.layer2.0.bn3.weight\", \"backbone.body.layer2.0.bn3.bias\", \"backbone.body.layer2.0.bn3.running_mean\", \"backbone.body.layer2.0.bn3.running_var\", \"backbone.body.layer2.0.downsample.0.weight\", \"backbone.body.layer2.0.downsample.1.weight\", \"backbone.body.layer2.0.downsample.1.bias\", \"backbone.body.layer2.0.downsample.1.running_mean\", \"backbone.body.layer2.0.downsample.1.running_var\", \"backbone.body.layer2.1.conv1.weight\", \"backbone.body.layer2.1.bn1.weight\", \"backbone.body.layer2.1.bn1.bias\", \"backbone.body.layer2.1.bn1.running_mean\", \"backbone.body.layer2.1.bn1.running_var\", \"backbone.body.layer2.1.conv2.weight\", \"backbone.body.layer2.1.bn2.weight\", \"backbone.body.layer2.1.bn2.bias\", \"backbone.body.layer2.1.bn2.running_mean\", \"backbone.body.layer2.1.bn2.running_var\", \"backbone.body.layer2.1.conv3.weight\", \"backbone.body.layer2.1.bn3.weight\", \"backbone.body.layer2.1.bn3.bias\", \"backbone.body.layer2.1.bn3.running_mean\", \"backbone.body.layer2.1.bn3.running_var\", \"backbone.body.layer2.2.conv1.weight\", \"backbone.body.layer2.2.bn1.weight\", \"backbone.body.layer2.2.bn1.bias\", \"backbone.body.layer2.2.bn1.running_mean\", \"backbone.body.layer2.2.bn1.running_var\", \"backbone.body.layer2.2.conv2.weight\", \"backbone.body.layer2.2.bn2.weight\", \"backbone.body.layer2.2.bn2.bias\", \"backbone.body.layer2.2.bn2.running_mean\", \"backbone.body.layer2.2.bn2.running_var\", \"backbone.body.layer2.2.conv3.weight\", \"backbone.body.layer2.2.bn3.weight\", \"backbone.body.layer2.2.bn3.bias\", \"backbone.body.layer2.2.bn3.running_mean\", \"backbone.body.layer2.2.bn3.running_var\", \"backbone.body.layer2.3.conv1.weight\", \"backbone.body.layer2.3.bn1.weight\", \"backbone.body.layer2.3.bn1.bias\", \"backbone.body.layer2.3.bn1.running_mean\", \"backbone.body.layer2.3.bn1.running_var\", \"backbone.body.layer2.3.conv2.weight\", \"backbone.body.layer2.3.bn2.weight\", \"backbone.body.layer2.3.bn2.bias\", \"backbone.body.layer2.3.bn2.running_mean\", \"backbone.body.layer2.3.bn2.running_var\", \"backbone.body.layer2.3.conv3.weight\", \"backbone.body.layer2.3.bn3.weight\", \"backbone.body.layer2.3.bn3.bias\", \"backbone.body.layer2.3.bn3.running_mean\", \"backbone.body.layer2.3.bn3.running_var\", \"backbone.body.layer3.0.conv1.weight\", \"backbone.body.layer3.0.bn1.weight\", \"backbone.body.layer3.0.bn1.bias\", \"backbone.body.layer3.0.bn1.running_mean\", \"backbone.body.layer3.0.bn1.running_var\", \"backbone.body.layer3.0.conv2.weight\", \"backbone.body.layer3.0.bn2.weight\", \"backbone.body.layer3.0.bn2.bias\", \"backbone.body.layer3.0.bn2.running_mean\", \"backbone.body.layer3.0.bn2.running_var\", \"backbone.body.layer3.0.conv3.weight\", \"backbone.body.layer3.0.bn3.weight\", \"backbone.body.layer3.0.bn3.bias\", \"backbone.body.layer3.0.bn3.running_mean\", \"backbone.body.layer3.0.bn3.running_var\", \"backbone.body.layer3.0.downsample.0.weight\", \"backbone.body.layer3.0.downsample.1.weight\", \"backbone.body.layer3.0.downsample.1.bias\", \"backbone.body.layer3.0.downsample.1.running_mean\", \"backbone.body.layer3.0.downsample.1.running_var\", \"backbone.body.layer3.1.conv1.weight\", \"backbone.body.layer3.1.bn1.weight\", \"backbone.body.layer3.1.bn1.bias\", \"backbone.body.layer3.1.bn1.running_mean\", \"backbone.body.layer3.1.bn1.running_var\", \"backbone.body.layer3.1.conv2.weight\", \"backbone.body.layer3.1.bn2.weight\", \"backbone.body.layer3.1.bn2.bias\", \"backbone.body.layer3.1.bn2.running_mean\", \"backbone.body.layer3.1.bn2.running_var\", \"backbone.body.layer3.1.conv3.weight\", \"backbone.body.layer3.1.bn3.weight\", \"backbone.body.layer3.1.bn3.bias\", \"backbone.body.layer3.1.bn3.running_mean\", \"backbone.body.layer3.1.bn3.running_var\", \"backbone.body.layer3.2.conv1.weight\", \"backbone.body.layer3.2.bn1.weight\", \"backbone.body.layer3.2.bn1.bias\", \"backbone.body.layer3.2.bn1.running_mean\", \"backbone.body.layer3.2.bn1.running_var\", \"backbone.body.layer3.2.conv2.weight\", \"backbone.body.layer3.2.bn2.weight\", \"backbone.body.layer3.2.bn2.bias\", \"backbone.body.layer3.2.bn2.running_mean\", \"backbone.body.layer3.2.bn2.running_var\", \"backbone.body.layer3.2.conv3.weight\", \"backbone.body.layer3.2.bn3.weight\", \"backbone.body.layer3.2.bn3.bias\", \"backbone.body.layer3.2.bn3.running_mean\", \"backbone.body.layer3.2.bn3.running_var\", \"backbone.body.layer3.3.conv1.weight\", \"backbone.body.layer3.3.bn1.weight\", \"backbone.body.layer3.3.bn1.bias\", \"backbone.body.layer3.3.bn1.running_mean\", \"backbone.body.layer3.3.bn1.running_var\", \"backbone.body.layer3.3.conv2.weight\", \"backbone.body.layer3.3.bn2.weight\", \"backbone.body.layer3.3.bn2.bias\", \"backbone.body.layer3.3.bn2.running_mean\", \"backbone.body.layer3.3.bn2.running_var\", \"backbone.body.layer3.3.conv3.weight\", \"backbone.body.layer3.3.bn3.weight\", \"backbone.body.layer3.3.bn3.bias\", \"backbone.body.layer3.3.bn3.running_mean\", \"backbone.body.layer3.3.bn3.running_var\", \"backbone.body.layer3.4.conv1.weight\", \"backbone.body.layer3.4.bn1.weight\", \"backbone.body.layer3.4.bn1.bias\", \"backbone.body.layer3.4.bn1.running_mean\", \"backbone.body.layer3.4.bn1.running_var\", \"backbone.body.layer3.4.conv2.weight\", \"backbone.body.layer3.4.bn2.weight\", \"backbone.body.layer3.4.bn2.bias\", \"backbone.body.layer3.4.bn2.running_mean\", \"backbone.body.layer3.4.bn2.running_var\", \"backbone.body.layer3.4.conv3.weight\", \"backbone.body.layer3.4.bn3.weight\", \"backbone.body.layer3.4.bn3.bias\", \"backbone.body.layer3.4.bn3.running_mean\", \"backbone.body.layer3.4.bn3.running_var\", \"backbone.body.layer3.5.conv1.weight\", \"backbone.body.layer3.5.bn1.weight\", \"backbone.body.layer3.5.bn1.bias\", \"backbone.body.layer3.5.bn1.running_mean\", \"backbone.body.layer3.5.bn1.running_var\", \"backbone.body.layer3.5.conv2.weight\", \"backbone.body.layer3.5.bn2.weight\", \"backbone.body.layer3.5.bn2.bias\", \"backbone.body.layer3.5.bn2.running_mean\", \"backbone.body.layer3.5.bn2.running_var\", \"backbone.body.layer3.5.conv3.weight\", \"backbone.body.layer3.5.bn3.weight\", \"backbone.body.layer3.5.bn3.bias\", \"backbone.body.layer3.5.bn3.running_mean\", \"backbone.body.layer3.5.bn3.running_var\", \"backbone.body.layer4.0.conv1.weight\", \"backbone.body.layer4.0.bn1.weight\", \"backbone.body.layer4.0.bn1.bias\", \"backbone.body.layer4.0.bn1.running_mean\", \"backbone.body.layer4.0.bn1.running_var\", \"backbone.body.layer4.0.conv2.weight\", \"backbone.body.layer4.0.bn2.weight\", \"backbone.body.layer4.0.bn2.bias\", \"backbone.body.layer4.0.bn2.running_mean\", \"backbone.body.layer4.0.bn2.running_var\", \"backbone.body.layer4.0.conv3.weight\", \"backbone.body.layer4.0.bn3.weight\", \"backbone.body.layer4.0.bn3.bias\", \"backbone.body.layer4.0.bn3.running_mean\", \"backbone.body.layer4.0.bn3.running_var\", \"backbone.body.layer4.0.downsample.0.weight\", \"backbone.body.layer4.0.downsample.1.weight\", \"backbone.body.layer4.0.downsample.1.bias\", \"backbone.body.layer4.0.downsample.1.running_mean\", \"backbone.body.layer4.0.downsample.1.running_var\", \"backbone.body.layer4.1.conv1.weight\", \"backbone.body.layer4.1.bn1.weight\", \"backbone.body.layer4.1.bn1.bias\", \"backbone.body.layer4.1.bn1.running_mean\", \"backbone.body.layer4.1.bn1.running_var\", \"backbone.body.layer4.1.conv2.weight\", \"backbone.body.layer4.1.bn2.weight\", \"backbone.body.layer4.1.bn2.bias\", \"backbone.body.layer4.1.bn2.running_mean\", \"backbone.body.layer4.1.bn2.running_var\", \"backbone.body.layer4.1.conv3.weight\", \"backbone.body.layer4.1.bn3.weight\", \"backbone.body.layer4.1.bn3.bias\", \"backbone.body.layer4.1.bn3.running_mean\", \"backbone.body.layer4.1.bn3.running_var\", \"backbone.body.layer4.2.conv1.weight\", \"backbone.body.layer4.2.bn1.weight\", \"backbone.body.layer4.2.bn1.bias\", \"backbone.body.layer4.2.bn1.running_mean\", \"backbone.body.layer4.2.bn1.running_var\", \"backbone.body.layer4.2.conv2.weight\", \"backbone.body.layer4.2.bn2.weight\", \"backbone.body.layer4.2.bn2.bias\", \"backbone.body.layer4.2.bn2.running_mean\", \"backbone.body.layer4.2.bn2.running_var\", \"backbone.body.layer4.2.conv3.weight\", \"backbone.body.layer4.2.bn3.weight\", \"backbone.body.layer4.2.bn3.bias\", \"backbone.body.layer4.2.bn3.running_mean\", \"backbone.body.layer4.2.bn3.running_var\", \"backbone.fpn.inner_blocks.0.0.weight\", \"backbone.fpn.inner_blocks.0.0.bias\", \"backbone.fpn.inner_blocks.1.0.weight\", \"backbone.fpn.inner_blocks.1.0.bias\", \"backbone.fpn.inner_blocks.2.0.weight\", \"backbone.fpn.inner_blocks.2.0.bias\", \"backbone.fpn.inner_blocks.3.0.weight\", \"backbone.fpn.inner_blocks.3.0.bias\", \"backbone.fpn.layer_blocks.0.0.weight\", \"backbone.fpn.layer_blocks.0.0.bias\", \"backbone.fpn.layer_blocks.1.0.weight\", \"backbone.fpn.layer_blocks.1.0.bias\", \"backbone.fpn.layer_blocks.2.0.weight\", \"backbone.fpn.layer_blocks.2.0.bias\", \"backbone.fpn.layer_blocks.3.0.weight\", \"backbone.fpn.layer_blocks.3.0.bias\", \"rpn.head.conv.0.0.weight\", \"rpn.head.conv.0.0.bias\", \"rpn.head.cls_logits.weight\", \"rpn.head.cls_logits.bias\", \"rpn.head.bbox_pred.weight\", \"rpn.head.bbox_pred.bias\", \"roi_heads.box_head.fc6.weight\", \"roi_heads.box_head.fc6.bias\", \"roi_heads.box_head.fc7.weight\", \"roi_heads.box_head.fc7.bias\", \"roi_heads.box_predictor.cls_score.weight\", \"roi_heads.box_predictor.cls_score.bias\", \"roi_heads.box_predictor.bbox_pred.weight\", \"roi_heads.box_predictor.bbox_pred.bias\". \n\tUnexpected key(s) in state_dict: \"date\", \"version\", \"license\", \"docs\", \"epoch\", \"best_fitness\", \"model\", \"ema\", \"updates\", \"optimizer\", \"train_args\", \"train_metrics\", \"train_results\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# background + weapon\u001b[39;00m\n\u001b[0;32m     22\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(num_classes)\n\u001b[1;32m---> 23\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     25\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\dsy92\\OneDrive\\Desktop\\Weapon-Detection\\new_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2580\u001b[0m             ),\n\u001b[0;32m   2581\u001b[0m         )\n\u001b[0;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2587\u001b[0m         )\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for FasterRCNN:\n\tMissing key(s) in state_dict: \"backbone.body.conv1.weight\", \"backbone.body.bn1.weight\", \"backbone.body.bn1.bias\", \"backbone.body.bn1.running_mean\", \"backbone.body.bn1.running_var\", \"backbone.body.layer1.0.conv1.weight\", \"backbone.body.layer1.0.bn1.weight\", \"backbone.body.layer1.0.bn1.bias\", \"backbone.body.layer1.0.bn1.running_mean\", \"backbone.body.layer1.0.bn1.running_var\", \"backbone.body.layer1.0.conv2.weight\", \"backbone.body.layer1.0.bn2.weight\", \"backbone.body.layer1.0.bn2.bias\", \"backbone.body.layer1.0.bn2.running_mean\", \"backbone.body.layer1.0.bn2.running_var\", \"backbone.body.layer1.0.conv3.weight\", \"backbone.body.layer1.0.bn3.weight\", \"backbone.body.layer1.0.bn3.bias\", \"backbone.body.layer1.0.bn3.running_mean\", \"backbone.body.layer1.0.bn3.running_var\", \"backbone.body.layer1.0.downsample.0.weight\", \"backbone.body.layer1.0.downsample.1.weight\", \"backbone.body.layer1.0.downsample.1.bias\", \"backbone.body.layer1.0.downsample.1.running_mean\", \"backbone.body.layer1.0.downsample.1.running_var\", \"backbone.body.layer1.1.conv1.weight\", \"backbone.body.layer1.1.bn1.weight\", \"backbone.body.layer1.1.bn1.bias\", \"backbone.body.layer1.1.bn1.running_mean\", \"backbone.body.layer1.1.bn1.running_var\", \"backbone.body.layer1.1.conv2.weight\", \"backbone.body.layer1.1.bn2.weight\", \"backbone.body.layer1.1.bn2.bias\", \"backbone.body.layer1.1.bn2.running_mean\", \"backbone.body.layer1.1.bn2.running_var\", \"backbone.body.layer1.1.conv3.weight\", \"backbone.body.layer1.1.bn3.weight\", \"backbone.body.layer1.1.bn3.bias\", \"backbone.body.layer1.1.bn3.running_mean\", \"backbone.body.layer1.1.bn3.running_var\", \"backbone.body.layer1.2.conv1.weight\", \"backbone.body.layer1.2.bn1.weight\", \"backbone.body.layer1.2.bn1.bias\", \"backbone.body.layer1.2.bn1.running_mean\", \"backbone.body.layer1.2.bn1.running_var\", \"backbone.body.layer1.2.conv2.weight\", \"backbone.body.layer1.2.bn2.weight\", \"backbone.body.layer1.2.bn2.bias\", \"backbone.body.layer1.2.bn2.running_mean\", \"backbone.body.layer1.2.bn2.running_var\", \"backbone.body.layer1.2.conv3.weight\", \"backbone.body.layer1.2.bn3.weight\", \"backbone.body.layer1.2.bn3.bias\", \"backbone.body.layer1.2.bn3.running_mean\", \"backbone.body.layer1.2.bn3.running_var\", \"backbone.body.layer2.0.conv1.weight\", \"backbone.body.layer2.0.bn1.weight\", \"backbone.body.layer2.0.bn1.bias\", \"backbone.body.layer2.0.bn1.running_mean\", \"backbone.body.layer2.0.bn1.running_var\", \"backbone.body.layer2.0.conv2.weight\", \"backbone.body.layer2.0.bn2.weight\", \"backbone.body.layer2.0.bn2.bias\", \"backbone.body.layer2.0.bn2.running_mean\", \"backbone.body.layer2.0.bn2.running_var\", \"backbone.body.layer2.0.conv3.weight\", \"backbone.body.layer2.0.bn3.weight\", \"backbone.body.layer2.0.bn3.bias\", \"backbone.body.layer2.0.bn3.running_mean\", \"backbone.body.layer2.0.bn3.running_var\", \"backbone.body.layer2.0.downsample.0.weight\", \"backbone.body.layer2.0.downsample.1.weight\", \"backbone.body.layer2.0.downsample.1.bias\", \"backbone.body.layer2.0.downsample.1.running_mean\", \"backbone.body.layer2.0.downsample.1.running_var\", \"backbone.body.layer2.1.conv1.weight\", \"backbone.body.layer2.1.bn1.weight\", \"backbone.body.layer2.1.bn1.bias\", \"backbone.body.layer2.1.bn1.running_mean\", \"backbone.body.layer2.1.bn1.running_var\", \"backbone.body.layer2.1.conv2.weight\", \"backbone.body.layer2.1.bn2.weight\", \"backbone.body.layer2.1.bn2.bias\", \"backbone.body.layer2.1.bn2.running_mean\", \"backbone.body.layer2.1.bn2.running_var\", \"backbone.body.layer2.1.conv3.weight\", \"backbone.body.layer2.1.bn3.weight\", \"backbone.body.layer2.1.bn3.bias\", \"backbone.body.layer2.1.bn3.running_mean\", \"backbone.body.layer2.1.bn3.running_var\", \"backbone.body.layer2.2.conv1.weight\", \"backbone.body.layer2.2.bn1.weight\", \"backbone.body.layer2.2.bn1.bias\", \"backbone.body.layer2.2.bn1.running_mean\", \"backbone.body.layer2.2.bn1.running_var\", \"backbone.body.layer2.2.conv2.weight\", \"backbone.body.layer2.2.bn2.weight\", \"backbone.body.layer2.2.bn2.bias\", \"backbone.body.layer2.2.bn2.running_mean\", \"backbone.body.layer2.2.bn2.running_var\", \"backbone.body.layer2.2.conv3.weight\", \"backbone.body.layer2.2.bn3.weight\", \"backbone.body.layer2.2.bn3.bias\", \"backbone.body.layer2.2.bn3.running_mean\", \"backbone.body.layer2.2.bn3.running_var\", \"backbone.body.layer2.3.conv1.weight\", \"backbone.body.layer2.3.bn1.weight\", \"backbone.body.layer2.3.bn1.bias\", \"backbone.body.layer2.3.bn1.running_mean\", \"backbone.body.layer2.3.bn1.running_var\", \"backbone.body.layer2.3.conv2.weight\", \"backbone.body.layer2.3.bn2.weight\", \"backbone.body.layer2.3.bn2.bias\", \"backbone.body.layer2.3.bn2.running_mean\", \"backbone.body.layer2.3.bn2.running_var\", \"backbone.body.layer2.3.conv3.weight\", \"backbone.body.layer2.3.bn3.weight\", \"backbone.body.layer2.3.bn3.bias\", \"backbone.body.layer2.3.bn3.running_mean\", \"backbone.body.layer2.3.bn3.running_var\", \"backbone.body.layer3.0.conv1.weight\", \"backbone.body.layer3.0.bn1.weight\", \"backbone.body.layer3.0.bn1.bias\", \"backbone.body.layer3.0.bn1.running_mean\", \"backbone.body.layer3.0.bn1.running_var\", \"backbone.body.layer3.0.conv2.weight\", \"backbone.body.layer3.0.bn2.weight\", \"backbone.body.layer3.0.bn2.bias\", \"backbone.body.layer3.0.bn2.running_mean\", \"backbone.body.layer3.0.bn2.running_var\", \"backbone.body.layer3.0.conv3.weight\", \"backbone.body.layer3.0.bn3.weight\", \"backbone.body.layer3.0.bn3.bias\", \"backbone.body.layer3.0.bn3.running_mean\", \"backbone.body.layer3.0.bn3.running_var\", \"backbone.body.layer3.0.downsample.0.weight\", \"backbone.body.layer3.0.downsample.1.weight\", \"backbone.body.layer3.0.downsample.1.bias\", \"backbone.body.layer3.0.downsample.1.running_mean\", \"backbone.body.layer3.0.downsample.1.running_var\", \"backbone.body.layer3.1.conv1.weight\", \"backbone.body.layer3.1.bn1.weight\", \"backbone.body.layer3.1.bn1.bias\", \"backbone.body.layer3.1.bn1.running_mean\", \"backbone.body.layer3.1.bn1.running_var\", \"backbone.body.layer3.1.conv2.weight\", \"backbone.body.layer3.1.bn2.weight\", \"backbone.body.layer3.1.bn2.bias\", \"backbone.body.layer3.1.bn2.running_mean\", \"backbone.body.layer3.1.bn2.running_var\", \"backbone.body.layer3.1.conv3.weight\", \"backbone.body.layer3.1.bn3.weight\", \"backbone.body.layer3.1.bn3.bias\", \"backbone.body.layer3.1.bn3.running_mean\", \"backbone.body.layer3.1.bn3.running_var\", \"backbone.body.layer3.2.conv1.weight\", \"backbone.body.layer3.2.bn1.weight\", \"backbone.body.layer3.2.bn1.bias\", \"backbone.body.layer3.2.bn1.running_mean\", \"backbone.body.layer3.2.bn1.running_var\", \"backbone.body.layer3.2.conv2.weight\", \"backbone.body.layer3.2.bn2.weight\", \"backbone.body.layer3.2.bn2.bias\", \"backbone.body.layer3.2.bn2.running_mean\", \"backbone.body.layer3.2.bn2.running_var\", \"backbone.body.layer3.2.conv3.weight\", \"backbone.body.layer3.2.bn3.weight\", \"backbone.body.layer3.2.bn3.bias\", \"backbone.body.layer3.2.bn3.running_mean\", \"backbone.body.layer3.2.bn3.running_var\", \"backbone.body.layer3.3.conv1.weight\", \"backbone.body.layer3.3.bn1.weight\", \"backbone.body.layer3.3.bn1.bias\", \"backbone.body.layer3.3.bn1.running_mean\", \"backbone.body.layer3.3.bn1.running_var\", \"backbone.body.layer3.3.conv2.weight\", \"backbone.body.layer3.3.bn2.weight\", \"backbone.body.layer3.3.bn2.bias\", \"backbone.body.layer3.3.bn2.running_mean\", \"backbone.body.layer3.3.bn2.running_var\", \"backbone.body.layer3.3.conv3.weight\", \"backbone.body.layer3.3.bn3.weight\", \"backbone.body.layer3.3.bn3.bias\", \"backbone.body.layer3.3.bn3.running_mean\", \"backbone.body.layer3.3.bn3.running_var\", \"backbone.body.layer3.4.conv1.weight\", \"backbone.body.layer3.4.bn1.weight\", \"backbone.body.layer3.4.bn1.bias\", \"backbone.body.layer3.4.bn1.running_mean\", \"backbone.body.layer3.4.bn1.running_var\", \"backbone.body.layer3.4.conv2.weight\", \"backbone.body.layer3.4.bn2.weight\", \"backbone.body.layer3.4.bn2.bias\", \"backbone.body.layer3.4.bn2.running_mean\", \"backbone.body.layer3.4.bn2.running_var\", \"backbone.body.layer3.4.conv3.weight\", \"backbone.body.layer3.4.bn3.weight\", \"backbone.body.layer3.4.bn3.bias\", \"backbone.body.layer3.4.bn3.running_mean\", \"backbone.body.layer3.4.bn3.running_var\", \"backbone.body.layer3.5.conv1.weight\", \"backbone.body.layer3.5.bn1.weight\", \"backbone.body.layer3.5.bn1.bias\", \"backbone.body.layer3.5.bn1.running_mean\", \"backbone.body.layer3.5.bn1.running_var\", \"backbone.body.layer3.5.conv2.weight\", \"backbone.body.layer3.5.bn2.weight\", \"backbone.body.layer3.5.bn2.bias\", \"backbone.body.layer3.5.bn2.running_mean\", \"backbone.body.layer3.5.bn2.running_var\", \"backbone.body.layer3.5.conv3.weight\", \"backbone.body.layer3.5.bn3.weight\", \"backbone.body.layer3.5.bn3.bias\", \"backbone.body.layer3.5.bn3.running_mean\", \"backbone.body.layer3.5.bn3.running_var\", \"backbone.body.layer4.0.conv1.weight\", \"backbone.body.layer4.0.bn1.weight\", \"backbone.body.layer4.0.bn1.bias\", \"backbone.body.layer4.0.bn1.running_mean\", \"backbone.body.layer4.0.bn1.running_var\", \"backbone.body.layer4.0.conv2.weight\", \"backbone.body.layer4.0.bn2.weight\", \"backbone.body.layer4.0.bn2.bias\", \"backbone.body.layer4.0.bn2.running_mean\", \"backbone.body.layer4.0.bn2.running_var\", \"backbone.body.layer4.0.conv3.weight\", \"backbone.body.layer4.0.bn3.weight\", \"backbone.body.layer4.0.bn3.bias\", \"backbone.body.layer4.0.bn3.running_mean\", \"backbone.body.layer4.0.bn3.running_var\", \"backbone.body.layer4.0.downsample.0.weight\", \"backbone.body.layer4.0.downsample.1.weight\", \"backbone.body.layer4.0.downsample.1.bias\", \"backbone.body.layer4.0.downsample.1.running_mean\", \"backbone.body.layer4.0.downsample.1.running_var\", \"backbone.body.layer4.1.conv1.weight\", \"backbone.body.layer4.1.bn1.weight\", \"backbone.body.layer4.1.bn1.bias\", \"backbone.body.layer4.1.bn1.running_mean\", \"backbone.body.layer4.1.bn1.running_var\", \"backbone.body.layer4.1.conv2.weight\", \"backbone.body.layer4.1.bn2.weight\", \"backbone.body.layer4.1.bn2.bias\", \"backbone.body.layer4.1.bn2.running_mean\", \"backbone.body.layer4.1.bn2.running_var\", \"backbone.body.layer4.1.conv3.weight\", \"backbone.body.layer4.1.bn3.weight\", \"backbone.body.layer4.1.bn3.bias\", \"backbone.body.layer4.1.bn3.running_mean\", \"backbone.body.layer4.1.bn3.running_var\", \"backbone.body.layer4.2.conv1.weight\", \"backbone.body.layer4.2.bn1.weight\", \"backbone.body.layer4.2.bn1.bias\", \"backbone.body.layer4.2.bn1.running_mean\", \"backbone.body.layer4.2.bn1.running_var\", \"backbone.body.layer4.2.conv2.weight\", \"backbone.body.layer4.2.bn2.weight\", \"backbone.body.layer4.2.bn2.bias\", \"backbone.body.layer4.2.bn2.running_mean\", \"backbone.body.layer4.2.bn2.running_var\", \"backbone.body.layer4.2.conv3.weight\", \"backbone.body.layer4.2.bn3.weight\", \"backbone.body.layer4.2.bn3.bias\", \"backbone.body.layer4.2.bn3.running_mean\", \"backbone.body.layer4.2.bn3.running_var\", \"backbone.fpn.inner_blocks.0.0.weight\", \"backbone.fpn.inner_blocks.0.0.bias\", \"backbone.fpn.inner_blocks.1.0.weight\", \"backbone.fpn.inner_blocks.1.0.bias\", \"backbone.fpn.inner_blocks.2.0.weight\", \"backbone.fpn.inner_blocks.2.0.bias\", \"backbone.fpn.inner_blocks.3.0.weight\", \"backbone.fpn.inner_blocks.3.0.bias\", \"backbone.fpn.layer_blocks.0.0.weight\", \"backbone.fpn.layer_blocks.0.0.bias\", \"backbone.fpn.layer_blocks.1.0.weight\", \"backbone.fpn.layer_blocks.1.0.bias\", \"backbone.fpn.layer_blocks.2.0.weight\", \"backbone.fpn.layer_blocks.2.0.bias\", \"backbone.fpn.layer_blocks.3.0.weight\", \"backbone.fpn.layer_blocks.3.0.bias\", \"rpn.head.conv.0.0.weight\", \"rpn.head.conv.0.0.bias\", \"rpn.head.cls_logits.weight\", \"rpn.head.cls_logits.bias\", \"rpn.head.bbox_pred.weight\", \"rpn.head.bbox_pred.bias\", \"roi_heads.box_head.fc6.weight\", \"roi_heads.box_head.fc6.bias\", \"roi_heads.box_head.fc7.weight\", \"roi_heads.box_head.fc7.bias\", \"roi_heads.box_predictor.cls_score.weight\", \"roi_heads.box_predictor.cls_score.bias\", \"roi_heads.box_predictor.bbox_pred.weight\", \"roi_heads.box_predictor.bbox_pred.bias\". \n\tUnexpected key(s) in state_dict: \"date\", \"version\", \"license\", \"docs\", \"epoch\", \"best_fitness\", \"model\", \"ema\", \"updates\", \"optimizer\", \"train_args\", \"train_metrics\", \"train_results\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from PIL import Image, ImageDraw\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Model path\n",
    "model_path = r\"C:\\Users\\dsy92\\OneDrive\\Desktop\\Weapon-Detection\\model\\version3.pt\"\n",
    "\n",
    "# Image path\n",
    "img_path = r\"C:\\Users\\dsy92\\OneDrive\\Desktop\\Weapon-Detection\\c2.jpeg\"\n",
    "\n",
    "# Load model\n",
    "def get_model(num_classes):\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = 2  # background + weapon\n",
    "model = get_model(num_classes)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Image transform\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Resize((800, 800)),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load and preprocess image\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "transformed_image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    prediction = model(transformed_image)\n",
    "\n",
    "# Draw boxes\n",
    "draw = ImageDraw.Draw(image)\n",
    "boxes = prediction[0]['boxes']\n",
    "scores = prediction[0]['scores']\n",
    "labels = prediction[0]['labels']\n",
    "\n",
    "# Draw boxes with score > 0.5\n",
    "for box, score, label in zip(boxes, scores, labels):\n",
    "    if score > 0.5:\n",
    "        draw.rectangle(box.tolist(), outline='red', width=3)\n",
    "        draw.text((box[0], box[1]), f\"Weapon {label.item()} ({score:.2f})\", fill='red')\n",
    "\n",
    "# Show image\n",
    "image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c54e72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\dsy92\\OneDrive\\Desktop\\Weapon-Detection\\c2.jpeg: 320x416 1 gun, 132.6ms\n",
      "Speed: 3.2ms preprocess, 132.6ms inference, 151.6ms postprocess per image at shape (1, 3, 320, 416)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'predicted_result.jpg'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "# import matplotlib.pyplot as plt\n",
    "# Load the trained YOLOv8 model\n",
    "model = YOLO(r\"C:\\Users\\dsy92\\OneDrive\\Desktop\\Weapon-Detection\\model\\version3.pt\")\n",
    "\n",
    "# Path to a test image\n",
    "image_path = r\"C:\\Users\\dsy92\\OneDrive\\Desktop\\Weapon-Detection\\c2.jpeg\"  # Replace with your image path\n",
    "\n",
    "# Run inference\n",
    "results = model(image_path)\n",
    "\n",
    "# Show the image with predictions\n",
    "results[0].show()\n",
    "\n",
    "# Optional: save the prediction result\n",
    "results[0].save(filename=\"predicted_result.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d40dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load the trained YOLOv8 model\n",
    "model = YOLO(r\"C:/Users/dsy92/OneDrive/Desktop/Weapon-Detection/model/version2.pt\")\n",
    "\n",
    "# Path to your test video\n",
    "video_path = r\"C:\\Users\\dsy92\\OneDrive\\Desktop\\Weapon-Detection\\models\\vt8.mp4\"  # Replace with your video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Optional: Save the output video\n",
    "output_path = \"predicted_output.avi\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run YOLOv8 inference on the frame\n",
    "    results = model(frame)\n",
    "\n",
    "    # Plot the results on the frame\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Weapon Detection\", annotated_frame)\n",
    "\n",
    "    # Write frame to output video\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release everything\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58967ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x416 (no detections), 2091.8ms\n",
      "Speed: 57.7ms preprocess, 2091.8ms inference, 20.6ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 18.2ms\n",
      "Speed: 3.2ms preprocess, 18.2ms inference, 32.3ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 14.1ms\n",
      "Speed: 2.0ms preprocess, 14.1ms inference, 4.8ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 14.6ms\n",
      "Speed: 1.9ms preprocess, 14.6ms inference, 4.2ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 15.5ms\n",
      "Speed: 2.0ms preprocess, 15.5ms inference, 2.2ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 13.2ms\n",
      "Speed: 2.3ms preprocess, 13.2ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 2087.8ms\n",
      "Speed: 2.8ms preprocess, 2087.8ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 44.0ms\n",
      "Speed: 1.9ms preprocess, 44.0ms inference, 5.4ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 29.0ms\n",
      "Speed: 2.0ms preprocess, 29.0ms inference, 5.5ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 13.2ms\n",
      "Speed: 1.9ms preprocess, 13.2ms inference, 3.1ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 20.9ms\n",
      "Speed: 1.4ms preprocess, 20.9ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 14.7ms\n",
      "Speed: 1.3ms preprocess, 14.7ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 20.7ms\n",
      "Speed: 1.5ms preprocess, 20.7ms inference, 2.2ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 11.3ms\n",
      "Speed: 1.2ms preprocess, 11.3ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 13.3ms\n",
      "Speed: 1.5ms preprocess, 13.3ms inference, 1.4ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 43.0ms\n",
      "Speed: 1.4ms preprocess, 43.0ms inference, 5.5ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 11.6ms\n",
      "Speed: 1.9ms preprocess, 11.6ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 22.2ms\n",
      "Speed: 1.9ms preprocess, 22.2ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 23.9ms\n",
      "Speed: 1.9ms preprocess, 23.9ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 25.6ms\n",
      "Speed: 3.0ms preprocess, 25.6ms inference, 1.9ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 17.8ms\n",
      "Speed: 1.6ms preprocess, 17.8ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 39.5ms\n",
      "Speed: 1.6ms preprocess, 39.5ms inference, 5.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 13.7ms\n",
      "Speed: 1.5ms preprocess, 13.7ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 24.4ms\n",
      "Speed: 1.9ms preprocess, 24.4ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 33.5ms\n",
      "Speed: 1.5ms preprocess, 33.5ms inference, 5.3ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 16.1ms\n",
      "Speed: 1.8ms preprocess, 16.1ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 39.9ms\n",
      "Speed: 3.5ms preprocess, 39.9ms inference, 4.9ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 15.2ms\n",
      "Speed: 2.5ms preprocess, 15.2ms inference, 1.6ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 11.7ms\n",
      "Speed: 1.7ms preprocess, 11.7ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 guns, 14.0ms\n",
      "Speed: 1.5ms preprocess, 14.0ms inference, 5.7ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 13.9ms\n",
      "Speed: 1.4ms preprocess, 13.9ms inference, 2.8ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 25.2ms\n",
      "Speed: 1.6ms preprocess, 25.2ms inference, 1.6ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 12.3ms\n",
      "Speed: 1.2ms preprocess, 12.3ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 15.2ms\n",
      "Speed: 1.4ms preprocess, 15.2ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 21.5ms\n",
      "Speed: 1.8ms preprocess, 21.5ms inference, 2.1ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 41.3ms\n",
      "Speed: 2.5ms preprocess, 41.3ms inference, 5.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 13.2ms\n",
      "Speed: 2.1ms preprocess, 13.2ms inference, 3.7ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 46.4ms\n",
      "Speed: 2.2ms preprocess, 46.4ms inference, 5.4ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 40.1ms\n",
      "Speed: 2.4ms preprocess, 40.1ms inference, 14.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 29.8ms\n",
      "Speed: 2.7ms preprocess, 29.8ms inference, 11.3ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 43.5ms\n",
      "Speed: 2.3ms preprocess, 43.5ms inference, 14.6ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 21.1ms\n",
      "Speed: 2.1ms preprocess, 21.1ms inference, 15.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 44.3ms\n",
      "Speed: 2.9ms preprocess, 44.3ms inference, 18.7ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 12.2ms\n",
      "Speed: 1.6ms preprocess, 12.2ms inference, 3.6ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 19.5ms\n",
      "Speed: 1.4ms preprocess, 19.5ms inference, 2.8ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 13.6ms\n",
      "Speed: 1.4ms preprocess, 13.6ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 13.9ms\n",
      "Speed: 1.3ms preprocess, 13.9ms inference, 2.6ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 37.6ms\n",
      "Speed: 1.7ms preprocess, 37.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 13.0ms\n",
      "Speed: 1.3ms preprocess, 13.0ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 20.4ms\n",
      "Speed: 1.5ms preprocess, 20.4ms inference, 5.1ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 36.4ms\n",
      "Speed: 3.0ms preprocess, 36.4ms inference, 13.3ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 28.6ms\n",
      "Speed: 2.7ms preprocess, 28.6ms inference, 13.5ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 40.7ms\n",
      "Speed: 2.8ms preprocess, 40.7ms inference, 14.9ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 31.5ms\n",
      "Speed: 2.9ms preprocess, 31.5ms inference, 5.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 20.8ms\n",
      "Speed: 2.0ms preprocess, 20.8ms inference, 15.1ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 37.2ms\n",
      "Speed: 2.0ms preprocess, 37.2ms inference, 14.8ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 35.4ms\n",
      "Speed: 2.4ms preprocess, 35.4ms inference, 13.2ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 14.3ms\n",
      "Speed: 2.3ms preprocess, 14.3ms inference, 8.2ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 15.2ms\n",
      "Speed: 2.6ms preprocess, 15.2ms inference, 2.2ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 14.2ms\n",
      "Speed: 1.3ms preprocess, 14.2ms inference, 2.4ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 10.5ms\n",
      "Speed: 1.3ms preprocess, 10.5ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 11.2ms\n",
      "Speed: 1.3ms preprocess, 11.2ms inference, 2.4ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 14.1ms\n",
      "Speed: 1.3ms preprocess, 14.1ms inference, 2.1ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 27.8ms\n",
      "Speed: 1.6ms preprocess, 27.8ms inference, 2.3ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 guns, 15.8ms\n",
      "Speed: 2.1ms preprocess, 15.8ms inference, 10.6ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 14.3ms\n",
      "Speed: 2.1ms preprocess, 14.3ms inference, 7.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 guns, 13.4ms\n",
      "Speed: 1.9ms preprocess, 13.4ms inference, 6.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 guns, 21.6ms\n",
      "Speed: 2.4ms preprocess, 21.6ms inference, 14.8ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 guns, 16.4ms\n",
      "Speed: 1.9ms preprocess, 16.4ms inference, 5.9ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 44.3ms\n",
      "Speed: 3.0ms preprocess, 44.3ms inference, 15.2ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 41.1ms\n",
      "Speed: 2.5ms preprocess, 41.1ms inference, 12.7ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 13.5ms\n",
      "Speed: 2.4ms preprocess, 13.5ms inference, 6.7ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 40.6ms\n",
      "Speed: 2.2ms preprocess, 40.6ms inference, 17.3ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 12.7ms\n",
      "Speed: 1.4ms preprocess, 12.7ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 13.9ms\n",
      "Speed: 2.2ms preprocess, 13.9ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 12.2ms\n",
      "Speed: 1.4ms preprocess, 12.2ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 36.3ms\n",
      "Speed: 1.9ms preprocess, 36.3ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 37.5ms\n",
      "Speed: 1.8ms preprocess, 37.5ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 15.3ms\n",
      "Speed: 1.5ms preprocess, 15.3ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 40.1ms\n",
      "Speed: 2.8ms preprocess, 40.1ms inference, 4.7ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 39.9ms\n",
      "Speed: 6.4ms preprocess, 39.9ms inference, 4.6ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 44.8ms\n",
      "Speed: 2.9ms preprocess, 44.8ms inference, 13.3ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 26.9ms\n",
      "Speed: 1.9ms preprocess, 26.9ms inference, 5.5ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 16.0ms\n",
      "Speed: 2.2ms preprocess, 16.0ms inference, 5.8ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 40.7ms\n",
      "Speed: 2.2ms preprocess, 40.7ms inference, 5.3ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 15.3ms\n",
      "Speed: 1.9ms preprocess, 15.3ms inference, 5.5ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 40.7ms\n",
      "Speed: 2.2ms preprocess, 40.7ms inference, 13.6ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 23.6ms\n",
      "Speed: 2.6ms preprocess, 23.6ms inference, 2.3ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 10.6ms\n",
      "Speed: 1.3ms preprocess, 10.6ms inference, 2.3ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 12.2ms\n",
      "Speed: 1.7ms preprocess, 12.2ms inference, 2.1ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 11.0ms\n",
      "Speed: 1.4ms preprocess, 11.0ms inference, 2.5ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 18.3ms\n",
      "Speed: 1.6ms preprocess, 18.3ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 14.1ms\n",
      "Speed: 1.7ms preprocess, 14.1ms inference, 2.4ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 13.9ms\n",
      "Speed: 1.7ms preprocess, 13.9ms inference, 3.4ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 42.5ms\n",
      "Speed: 1.9ms preprocess, 42.5ms inference, 14.9ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 50.2ms\n",
      "Speed: 2.5ms preprocess, 50.2ms inference, 5.9ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 33.2ms\n",
      "Speed: 2.5ms preprocess, 33.2ms inference, 11.2ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 49.5ms\n",
      "Speed: 3.0ms preprocess, 49.5ms inference, 5.2ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 35.7ms\n",
      "Speed: 2.2ms preprocess, 35.7ms inference, 4.5ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 27.7ms\n",
      "Speed: 2.3ms preprocess, 27.7ms inference, 4.4ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 34.5ms\n",
      "Speed: 2.3ms preprocess, 34.5ms inference, 5.4ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 34.7ms\n",
      "Speed: 2.8ms preprocess, 34.7ms inference, 6.8ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 29.6ms\n",
      "Speed: 2.0ms preprocess, 29.6ms inference, 1.6ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 19.7ms\n",
      "Speed: 1.6ms preprocess, 19.7ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 23.2ms\n",
      "Speed: 1867.5ms preprocess, 23.2ms inference, 3.9ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 40.3ms\n",
      "Speed: 2.8ms preprocess, 40.3ms inference, 5.9ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 20.4ms\n",
      "Speed: 3.5ms preprocess, 20.4ms inference, 3.2ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 36.8ms\n",
      "Speed: 2.3ms preprocess, 36.8ms inference, 5.4ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 47.1ms\n",
      "Speed: 2.4ms preprocess, 47.1ms inference, 18.3ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 15.4ms\n",
      "Speed: 1.8ms preprocess, 15.4ms inference, 4.2ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 18.7ms\n",
      "Speed: 1.5ms preprocess, 18.7ms inference, 1.9ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 15.8ms\n",
      "Speed: 1.6ms preprocess, 15.8ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 14.0ms\n",
      "Speed: 1.7ms preprocess, 14.0ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 38.3ms\n",
      "Speed: 1868.7ms preprocess, 38.3ms inference, 13.2ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 18.5ms\n",
      "Speed: 3.9ms preprocess, 18.5ms inference, 6.2ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 49.1ms\n",
      "Speed: 3.9ms preprocess, 49.1ms inference, 15.1ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 guns, 47.8ms\n",
      "Speed: 3.0ms preprocess, 47.8ms inference, 14.9ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 24.0ms\n",
      "Speed: 2.2ms preprocess, 24.0ms inference, 9.1ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 21.0ms\n",
      "Speed: 1.6ms preprocess, 21.0ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 12.2ms\n",
      "Speed: 1.8ms preprocess, 12.2ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 41.8ms\n",
      "Speed: 1.8ms preprocess, 41.8ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 23.4ms\n",
      "Speed: 1.6ms preprocess, 23.4ms inference, 3.6ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 13.3ms\n",
      "Speed: 1.5ms preprocess, 13.3ms inference, 2.2ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 14.6ms\n",
      "Speed: 1.9ms preprocess, 14.6ms inference, 3.4ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 43.9ms\n",
      "Speed: 1832.9ms preprocess, 43.9ms inference, 22.9ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 20.5ms\n",
      "Speed: 3.2ms preprocess, 20.5ms inference, 7.6ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 24.3ms\n",
      "Speed: 2.0ms preprocess, 24.3ms inference, 7.3ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 48.1ms\n",
      "Speed: 3.2ms preprocess, 48.1ms inference, 14.9ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 21.7ms\n",
      "Speed: 2.1ms preprocess, 21.7ms inference, 3.3ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 29.0ms\n",
      "Speed: 2.2ms preprocess, 29.0ms inference, 3.2ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 37.2ms\n",
      "Speed: 2.0ms preprocess, 37.2ms inference, 3.9ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 24.7ms\n",
      "Speed: 1.9ms preprocess, 24.7ms inference, 5.4ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 60.1ms\n",
      "Speed: 1737.0ms preprocess, 60.1ms inference, 14.3ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 81.5ms\n",
      "Speed: 6.7ms preprocess, 81.5ms inference, 7.7ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 73.6ms\n",
      "Speed: 4.4ms preprocess, 73.6ms inference, 8.3ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 26.2ms\n",
      "Speed: 2.2ms preprocess, 26.2ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 14.5ms\n",
      "Speed: 1.5ms preprocess, 14.5ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 24.8ms\n",
      "Speed: 1.5ms preprocess, 24.8ms inference, 2.8ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 guns, 26.0ms\n",
      "Speed: 1.4ms preprocess, 26.0ms inference, 3.4ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 guns, 27.1ms\n",
      "Speed: 1.6ms preprocess, 27.1ms inference, 5.2ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 guns, 17.0ms\n",
      "Speed: 1.2ms preprocess, 17.0ms inference, 3.3ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 guns, 22.9ms\n",
      "Speed: 1920.2ms preprocess, 22.9ms inference, 5.3ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 guns, 45.4ms\n",
      "Speed: 4.7ms preprocess, 45.4ms inference, 15.5ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 guns, 44.6ms\n",
      "Speed: 3.1ms preprocess, 44.6ms inference, 17.2ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 guns, 11.4ms\n",
      "Speed: 1.8ms preprocess, 11.4ms inference, 2.9ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 guns, 13.0ms\n",
      "Speed: 1.3ms preprocess, 13.0ms inference, 2.3ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 guns, 15.9ms\n",
      "Speed: 1.5ms preprocess, 15.9ms inference, 3.9ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 guns, 12.7ms\n",
      "Speed: 1.8ms preprocess, 12.7ms inference, 4.6ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 guns, 13.7ms\n",
      "Speed: 1.5ms preprocess, 13.7ms inference, 2.5ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 guns, 13.4ms\n",
      "Speed: 1.4ms preprocess, 13.4ms inference, 2.5ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 guns, 43.5ms\n",
      "Speed: 2002.1ms preprocess, 43.5ms inference, 15.3ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 guns, 15.6ms\n",
      "Speed: 2.0ms preprocess, 15.6ms inference, 7.1ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 guns, 13.0ms\n",
      "Speed: 3.1ms preprocess, 13.0ms inference, 2.4ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 gun, 13.6ms\n",
      "Speed: 1.3ms preprocess, 13.6ms inference, 3.7ms postprocess per image at shape (1, 3, 224, 416)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load the trained YOLOv8 model\n",
    "model = YOLO(r\"C:/Users/dsy92/OneDrive/Desktop/Weapon-Detection/model/version3.pt\")\n",
    "\n",
    "# Path to your test video\n",
    "video_path = r\"C:\\Users\\dsy92\\OneDrive\\Desktop\\Weapon-Detection\\vd9.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Get video properties\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Output video writer\n",
    "out = cv2.VideoWriter(\"output_with_predictions.avi\", cv2.VideoWriter_fourcc(*'XVID'), fps, (width, height))\n",
    "\n",
    "# Process video\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run inference\n",
    "    results = model(frame)\n",
    "\n",
    "    # Draw results on the frame\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"YOLOv8 Weapon Detection\", annotated_frame)\n",
    "\n",
    "    # Write to output video\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "    # Press 'q' to exit early\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93da9019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 416x224 (no detections), 90.7ms\n",
      "Speed: 7.3ms preprocess, 90.7ms inference, 15.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.1ms\n",
      "Speed: 1.7ms preprocess, 12.1ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.3ms\n",
      "Speed: 1.3ms preprocess, 12.3ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.1ms\n",
      "Speed: 1.3ms preprocess, 13.1ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.2ms\n",
      "Speed: 1.3ms preprocess, 13.2ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.3ms\n",
      "Speed: 2.1ms preprocess, 11.3ms inference, 2.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.7ms\n",
      "Speed: 1.6ms preprocess, 12.7ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.6ms\n",
      "Speed: 1.3ms preprocess, 12.6ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.2ms\n",
      "Speed: 1.4ms preprocess, 11.2ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.6ms\n",
      "Speed: 1.5ms preprocess, 11.6ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.3ms\n",
      "Speed: 1.3ms preprocess, 12.3ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.9ms\n",
      "Speed: 1.4ms preprocess, 11.9ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.1ms\n",
      "Speed: 1.5ms preprocess, 11.1ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 10.9ms\n",
      "Speed: 1.6ms preprocess, 10.9ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.1ms\n",
      "Speed: 1.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.2ms\n",
      "Speed: 1.6ms preprocess, 11.2ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.0ms\n",
      "Speed: 1.3ms preprocess, 12.0ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 14.4ms\n",
      "Speed: 1.4ms preprocess, 14.4ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.5ms\n",
      "Speed: 1.5ms preprocess, 12.5ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.9ms\n",
      "Speed: 1.7ms preprocess, 13.9ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 10.8ms\n",
      "Speed: 1.7ms preprocess, 10.8ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.4ms\n",
      "Speed: 2.2ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.1ms\n",
      "Speed: 1.8ms preprocess, 13.1ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.3ms\n",
      "Speed: 1.3ms preprocess, 11.3ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.2ms\n",
      "Speed: 1.4ms preprocess, 13.2ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.8ms\n",
      "Speed: 1.5ms preprocess, 11.8ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.2ms\n",
      "Speed: 2.6ms preprocess, 18.2ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.3ms\n",
      "Speed: 1.4ms preprocess, 11.3ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.4ms\n",
      "Speed: 1.8ms preprocess, 12.4ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.8ms\n",
      "Speed: 2.9ms preprocess, 15.8ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.1ms\n",
      "Speed: 2.4ms preprocess, 15.1ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 10.7ms\n",
      "Speed: 2.0ms preprocess, 10.7ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.0ms\n",
      "Speed: 1.8ms preprocess, 11.0ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.3ms\n",
      "Speed: 1.8ms preprocess, 12.3ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 10.6ms\n",
      "Speed: 2.1ms preprocess, 10.6ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 10.4ms\n",
      "Speed: 1.8ms preprocess, 10.4ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.9ms\n",
      "Speed: 1.6ms preprocess, 11.9ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.3ms\n",
      "Speed: 2.1ms preprocess, 12.3ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.6ms\n",
      "Speed: 1.4ms preprocess, 11.6ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.1ms\n",
      "Speed: 1.8ms preprocess, 11.1ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.0ms\n",
      "Speed: 1.8ms preprocess, 12.0ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.3ms\n",
      "Speed: 3.2ms preprocess, 16.3ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.2ms\n",
      "Speed: 1.4ms preprocess, 13.2ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.8ms\n",
      "Speed: 2.4ms preprocess, 15.8ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.2ms\n",
      "Speed: 2.0ms preprocess, 16.2ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.7ms\n",
      "Speed: 1.5ms preprocess, 13.7ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.3ms\n",
      "Speed: 1.6ms preprocess, 12.3ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 27.3ms\n",
      "Speed: 3.5ms preprocess, 27.3ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 14.0ms\n",
      "Speed: 3.6ms preprocess, 14.0ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.0ms\n",
      "Speed: 2.4ms preprocess, 11.0ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.2ms\n",
      "Speed: 1.7ms preprocess, 18.2ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.0ms\n",
      "Speed: 1.8ms preprocess, 11.0ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.9ms\n",
      "Speed: 1.5ms preprocess, 12.9ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.5ms\n",
      "Speed: 1.5ms preprocess, 12.5ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.4ms\n",
      "Speed: 1.9ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.6ms\n",
      "Speed: 1.9ms preprocess, 11.6ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.3ms\n",
      "Speed: 1.4ms preprocess, 17.3ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.5ms\n",
      "Speed: 1.5ms preprocess, 16.5ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.4ms\n",
      "Speed: 1.8ms preprocess, 16.4ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.1ms\n",
      "Speed: 1.6ms preprocess, 16.1ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.5ms\n",
      "Speed: 1.7ms preprocess, 15.5ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.8ms\n",
      "Speed: 1.5ms preprocess, 15.8ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 10.8ms\n",
      "Speed: 1.8ms preprocess, 10.8ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.0ms\n",
      "Speed: 2.1ms preprocess, 11.0ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.0ms\n",
      "Speed: 1.6ms preprocess, 19.0ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.0ms\n",
      "Speed: 1.5ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.3ms\n",
      "Speed: 1.4ms preprocess, 11.3ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 10.9ms\n",
      "Speed: 1.4ms preprocess, 10.9ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.1ms\n",
      "Speed: 1.8ms preprocess, 11.1ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 10.6ms\n",
      "Speed: 1.6ms preprocess, 10.6ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.9ms\n",
      "Speed: 2.0ms preprocess, 15.9ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.0ms\n",
      "Speed: 1.4ms preprocess, 12.0ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 10.9ms\n",
      "Speed: 1.7ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.0ms\n",
      "Speed: 1.6ms preprocess, 13.0ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.3ms\n",
      "Speed: 1.7ms preprocess, 17.3ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.9ms\n",
      "Speed: 1.8ms preprocess, 15.9ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.0ms\n",
      "Speed: 2.6ms preprocess, 16.0ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.6ms\n",
      "Speed: 2.3ms preprocess, 12.6ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 10.4ms\n",
      "Speed: 1.6ms preprocess, 10.4ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 10.1ms\n",
      "Speed: 2.0ms preprocess, 10.1ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.3ms\n",
      "Speed: 1.4ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.6ms\n",
      "Speed: 1.7ms preprocess, 12.6ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 10.4ms\n",
      "Speed: 1.6ms preprocess, 10.4ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 10.6ms\n",
      "Speed: 1.5ms preprocess, 10.6ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 10.6ms\n",
      "Speed: 1.7ms preprocess, 10.6ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 10.2ms\n",
      "Speed: 1.6ms preprocess, 10.2ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.9ms\n",
      "Speed: 1.5ms preprocess, 11.9ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.2ms\n",
      "Speed: 1.5ms preprocess, 11.2ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 10.7ms\n",
      "Speed: 1.5ms preprocess, 10.7ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 27.3ms\n",
      "Speed: 2.9ms preprocess, 27.3ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.8ms\n",
      "Speed: 1.4ms preprocess, 11.8ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.4ms\n",
      "Speed: 2.0ms preprocess, 17.4ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.9ms\n",
      "Speed: 1.7ms preprocess, 18.9ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.6ms\n",
      "Speed: 1.5ms preprocess, 17.6ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.0ms\n",
      "Speed: 2.4ms preprocess, 19.0ms inference, 2.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 28.1ms\n",
      "Speed: 1.9ms preprocess, 28.1ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.3ms\n",
      "Speed: 1.7ms preprocess, 19.3ms inference, 2.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.7ms\n",
      "Speed: 2.4ms preprocess, 18.7ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 21.4ms\n",
      "Speed: 2.2ms preprocess, 21.4ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 20.4ms\n",
      "Speed: 2.9ms preprocess, 20.4ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.1ms\n",
      "Speed: 2.5ms preprocess, 18.1ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.4ms\n",
      "Speed: 2.0ms preprocess, 19.4ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 20.1ms\n",
      "Speed: 2.3ms preprocess, 20.1ms inference, 2.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.1ms\n",
      "Speed: 2.5ms preprocess, 18.1ms inference, 2.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.7ms\n",
      "Speed: 2.0ms preprocess, 18.7ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.4ms\n",
      "Speed: 2.4ms preprocess, 18.4ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.8ms\n",
      "Speed: 1.4ms preprocess, 11.8ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.3ms\n",
      "Speed: 2.2ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 10.8ms\n",
      "Speed: 1.6ms preprocess, 10.8ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.7ms\n",
      "Speed: 1.5ms preprocess, 11.7ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 14.4ms\n",
      "Speed: 2.4ms preprocess, 14.4ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.1ms\n",
      "Speed: 1.9ms preprocess, 12.1ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 10.8ms\n",
      "Speed: 2.1ms preprocess, 10.8ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.3ms\n",
      "Speed: 1.5ms preprocess, 11.3ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.9ms\n",
      "Speed: 1.6ms preprocess, 11.9ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 10.7ms\n",
      "Speed: 1.5ms preprocess, 10.7ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.0ms\n",
      "Speed: 1.5ms preprocess, 11.0ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 10.9ms\n",
      "Speed: 2.1ms preprocess, 10.9ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.3ms\n",
      "Speed: 1.6ms preprocess, 11.3ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 10.7ms\n",
      "Speed: 2.2ms preprocess, 10.7ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.5ms\n",
      "Speed: 1.5ms preprocess, 17.5ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.8ms\n",
      "Speed: 2.3ms preprocess, 17.8ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.7ms\n",
      "Speed: 1.5ms preprocess, 18.7ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.1ms\n",
      "Speed: 1.8ms preprocess, 18.1ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.2ms\n",
      "Speed: 1.8ms preprocess, 18.2ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.3ms\n",
      "Speed: 2.0ms preprocess, 18.3ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.3ms\n",
      "Speed: 2.1ms preprocess, 17.3ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.5ms\n",
      "Speed: 1.6ms preprocess, 18.5ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.5ms\n",
      "Speed: 1.7ms preprocess, 19.5ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.4ms\n",
      "Speed: 3.5ms preprocess, 18.4ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.8ms\n",
      "Speed: 1.7ms preprocess, 19.8ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.6ms\n",
      "Speed: 2.0ms preprocess, 18.6ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.3ms\n",
      "Speed: 1.9ms preprocess, 19.3ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.8ms\n",
      "Speed: 2.5ms preprocess, 18.8ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.0ms\n",
      "Speed: 2.2ms preprocess, 19.0ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.7ms\n",
      "Speed: 1.8ms preprocess, 18.7ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.6ms\n",
      "Speed: 2.1ms preprocess, 18.6ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 44.2ms\n",
      "Speed: 1.9ms preprocess, 44.2ms inference, 4.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.9ms\n",
      "Speed: 2.5ms preprocess, 17.9ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.9ms\n",
      "Speed: 2.4ms preprocess, 18.9ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.2ms\n",
      "Speed: 2.0ms preprocess, 19.2ms inference, 2.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.1ms\n",
      "Speed: 1.9ms preprocess, 19.1ms inference, 2.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.2ms\n",
      "Speed: 1.9ms preprocess, 19.2ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.0ms\n",
      "Speed: 1.8ms preprocess, 19.0ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.2ms\n",
      "Speed: 2.1ms preprocess, 19.2ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.0ms\n",
      "Speed: 1.9ms preprocess, 19.0ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.1ms\n",
      "Speed: 2.8ms preprocess, 19.1ms inference, 2.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.4ms\n",
      "Speed: 1.9ms preprocess, 19.4ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.3ms\n",
      "Speed: 2.1ms preprocess, 19.3ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 20.1ms\n",
      "Speed: 2.7ms preprocess, 20.1ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.7ms\n",
      "Speed: 1.7ms preprocess, 19.7ms inference, 2.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.8ms\n",
      "Speed: 2.3ms preprocess, 19.8ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.9ms\n",
      "Speed: 2.9ms preprocess, 19.9ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 20.4ms\n",
      "Speed: 2.8ms preprocess, 20.4ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 28.6ms\n",
      "Speed: 1.9ms preprocess, 28.6ms inference, 2.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 39.5ms\n",
      "Speed: 2.0ms preprocess, 39.5ms inference, 2.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.0ms\n",
      "Speed: 1.8ms preprocess, 17.0ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.6ms\n",
      "Speed: 2.2ms preprocess, 16.6ms inference, 2.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.8ms\n",
      "Speed: 2.0ms preprocess, 16.8ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.7ms\n",
      "Speed: 2.0ms preprocess, 16.7ms inference, 2.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.9ms\n",
      "Speed: 2.1ms preprocess, 16.9ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.7ms\n",
      "Speed: 1.7ms preprocess, 16.7ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.1ms\n",
      "Speed: 1.8ms preprocess, 17.1ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.9ms\n",
      "Speed: 4.0ms preprocess, 16.9ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.8ms\n",
      "Speed: 3.4ms preprocess, 16.8ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.7ms\n",
      "Speed: 1.6ms preprocess, 16.7ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.6ms\n",
      "Speed: 3.6ms preprocess, 16.6ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.8ms\n",
      "Speed: 4.0ms preprocess, 16.8ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.9ms\n",
      "Speed: 2.2ms preprocess, 16.9ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.3ms\n",
      "Speed: 2.0ms preprocess, 18.3ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.1ms\n",
      "Speed: 2.0ms preprocess, 17.1ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.4ms\n",
      "Speed: 1.7ms preprocess, 18.4ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.4ms\n",
      "Speed: 1.9ms preprocess, 17.4ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.7ms\n",
      "Speed: 1.9ms preprocess, 17.7ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.5ms\n",
      "Speed: 3.0ms preprocess, 17.5ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.5ms\n",
      "Speed: 1.7ms preprocess, 17.5ms inference, 2.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.7ms\n",
      "Speed: 2.3ms preprocess, 17.7ms inference, 2.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.4ms\n",
      "Speed: 2.0ms preprocess, 17.4ms inference, 2.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.7ms\n",
      "Speed: 2.7ms preprocess, 17.7ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.8ms\n",
      "Speed: 2.1ms preprocess, 17.8ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.9ms\n",
      "Speed: 3.3ms preprocess, 17.9ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.7ms\n",
      "Speed: 1.9ms preprocess, 18.7ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.8ms\n",
      "Speed: 1.9ms preprocess, 17.8ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.2ms\n",
      "Speed: 2.0ms preprocess, 18.2ms inference, 4.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.2ms\n",
      "Speed: 1.9ms preprocess, 18.2ms inference, 2.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.3ms\n",
      "Speed: 1.8ms preprocess, 18.3ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.2ms\n",
      "Speed: 3.5ms preprocess, 19.2ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.0ms\n",
      "Speed: 2.1ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.6ms\n",
      "Speed: 1.7ms preprocess, 18.6ms inference, 3.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.7ms\n",
      "Speed: 2.1ms preprocess, 18.7ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.9ms\n",
      "Speed: 2.3ms preprocess, 18.9ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 21.9ms\n",
      "Speed: 2.1ms preprocess, 21.9ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.7ms\n",
      "Speed: 2.2ms preprocess, 18.7ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.7ms\n",
      "Speed: 2.2ms preprocess, 18.7ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.4ms\n",
      "Speed: 2.3ms preprocess, 18.4ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.7ms\n",
      "Speed: 3.2ms preprocess, 17.7ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.6ms\n",
      "Speed: 1.8ms preprocess, 17.6ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.7ms\n",
      "Speed: 1.9ms preprocess, 18.7ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.8ms\n",
      "Speed: 1.8ms preprocess, 18.8ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.5ms\n",
      "Speed: 1.9ms preprocess, 18.5ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.9ms\n",
      "Speed: 2.2ms preprocess, 18.9ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.6ms\n",
      "Speed: 2.8ms preprocess, 19.6ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.4ms\n",
      "Speed: 1.9ms preprocess, 18.4ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.5ms\n",
      "Speed: 1.8ms preprocess, 18.5ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.2ms\n",
      "Speed: 2.4ms preprocess, 18.2ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.0ms\n",
      "Speed: 2.5ms preprocess, 18.0ms inference, 2.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.4ms\n",
      "Speed: 1.9ms preprocess, 18.4ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.2ms\n",
      "Speed: 2.2ms preprocess, 18.2ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.2ms\n",
      "Speed: 1.9ms preprocess, 18.2ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.3ms\n",
      "Speed: 2.0ms preprocess, 18.3ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.4ms\n",
      "Speed: 1.7ms preprocess, 18.4ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.1ms\n",
      "Speed: 4.4ms preprocess, 18.1ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.1ms\n",
      "Speed: 1.8ms preprocess, 18.1ms inference, 2.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.3ms\n",
      "Speed: 2.0ms preprocess, 18.3ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.2ms\n",
      "Speed: 3.3ms preprocess, 18.2ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.6ms\n",
      "Speed: 1.9ms preprocess, 18.6ms inference, 2.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.6ms\n",
      "Speed: 2.0ms preprocess, 17.6ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.0ms\n",
      "Speed: 1.9ms preprocess, 19.0ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.7ms\n",
      "Speed: 2.2ms preprocess, 19.7ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 1 gun, 18.6ms\n",
      "Speed: 2.5ms preprocess, 18.6ms inference, 72.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.6ms\n",
      "Speed: 2.2ms preprocess, 18.6ms inference, 2.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.1ms\n",
      "Speed: 1.8ms preprocess, 19.1ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.9ms\n",
      "Speed: 1.8ms preprocess, 18.9ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.3ms\n",
      "Speed: 2.2ms preprocess, 19.3ms inference, 2.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.6ms\n",
      "Speed: 2.6ms preprocess, 19.6ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 20.1ms\n",
      "Speed: 1.7ms preprocess, 20.1ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.3ms\n",
      "Speed: 1.9ms preprocess, 19.3ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 2 guns, 19.7ms\n",
      "Speed: 1.7ms preprocess, 19.7ms inference, 18.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 2 guns, 19.4ms\n",
      "Speed: 2.5ms preprocess, 19.4ms inference, 3.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 2 guns, 19.7ms\n",
      "Speed: 2.5ms preprocess, 19.7ms inference, 2.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 1 gun, 28.3ms\n",
      "Speed: 3.1ms preprocess, 28.3ms inference, 8.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 1 gun, 19.5ms\n",
      "Speed: 2.3ms preprocess, 19.5ms inference, 3.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.6ms\n",
      "Speed: 2.9ms preprocess, 19.6ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 20.0ms\n",
      "Speed: 2.7ms preprocess, 20.0ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.4ms\n",
      "Speed: 2.2ms preprocess, 18.4ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.3ms\n",
      "Speed: 2.1ms preprocess, 19.3ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.7ms\n",
      "Speed: 3.5ms preprocess, 19.7ms inference, 2.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.1ms\n",
      "Speed: 1.8ms preprocess, 19.1ms inference, 2.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.6ms\n",
      "Speed: 2.1ms preprocess, 18.6ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.6ms\n",
      "Speed: 2.6ms preprocess, 18.6ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.2ms\n",
      "Speed: 2.5ms preprocess, 19.2ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.7ms\n",
      "Speed: 3.0ms preprocess, 19.7ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 20.7ms\n",
      "Speed: 2.6ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.3ms\n",
      "Speed: 2.0ms preprocess, 19.3ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.7ms\n",
      "Speed: 2.6ms preprocess, 19.7ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.3ms\n",
      "Speed: 2.2ms preprocess, 19.3ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.1ms\n",
      "Speed: 1.9ms preprocess, 19.1ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 39.9ms\n",
      "Speed: 2.4ms preprocess, 39.9ms inference, 2.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 27.9ms\n",
      "Speed: 2.0ms preprocess, 27.9ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.6ms\n",
      "Speed: 2.2ms preprocess, 18.6ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.5ms\n",
      "Speed: 2.0ms preprocess, 17.5ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.5ms\n",
      "Speed: 2.1ms preprocess, 18.5ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.6ms\n",
      "Speed: 2.1ms preprocess, 17.6ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 1 gun, 18.0ms\n",
      "Speed: 1.9ms preprocess, 18.0ms inference, 2.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.6ms\n",
      "Speed: 1.7ms preprocess, 17.6ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.1ms\n",
      "Speed: 2.0ms preprocess, 18.1ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.2ms\n",
      "Speed: 2.2ms preprocess, 18.2ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.6ms\n",
      "Speed: 1.9ms preprocess, 17.6ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 1 gun, 18.4ms\n",
      "Speed: 2.2ms preprocess, 18.4ms inference, 2.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.7ms\n",
      "Speed: 2.1ms preprocess, 17.7ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.1ms\n",
      "Speed: 1.9ms preprocess, 18.1ms inference, 2.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 1 gun, 17.9ms\n",
      "Speed: 1.9ms preprocess, 17.9ms inference, 2.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.6ms\n",
      "Speed: 1.9ms preprocess, 17.6ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.5ms\n",
      "Speed: 1.7ms preprocess, 19.5ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.0ms\n",
      "Speed: 2.2ms preprocess, 17.0ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 1 gun, 17.9ms\n",
      "Speed: 3.2ms preprocess, 17.9ms inference, 4.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 1 gun, 17.5ms\n",
      "Speed: 1.9ms preprocess, 17.5ms inference, 3.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 24.1ms\n",
      "Speed: 1.9ms preprocess, 24.1ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.9ms\n",
      "Speed: 4.1ms preprocess, 18.9ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.6ms\n",
      "Speed: 2.0ms preprocess, 17.6ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.2ms\n",
      "Speed: 1.9ms preprocess, 17.2ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.6ms\n",
      "Speed: 2.5ms preprocess, 17.6ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.0ms\n",
      "Speed: 2.5ms preprocess, 18.0ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.6ms\n",
      "Speed: 2.2ms preprocess, 17.6ms inference, 2.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.6ms\n",
      "Speed: 2.2ms preprocess, 17.6ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.9ms\n",
      "Speed: 2.2ms preprocess, 16.9ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.1ms\n",
      "Speed: 2.1ms preprocess, 18.1ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.2ms\n",
      "Speed: 2.0ms preprocess, 18.2ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.3ms\n",
      "Speed: 4.5ms preprocess, 18.3ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.6ms\n",
      "Speed: 2.0ms preprocess, 18.6ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.5ms\n",
      "Speed: 2.2ms preprocess, 19.5ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.6ms\n",
      "Speed: 2.0ms preprocess, 18.6ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.7ms\n",
      "Speed: 1.9ms preprocess, 18.7ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.4ms\n",
      "Speed: 1.7ms preprocess, 19.4ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.4ms\n",
      "Speed: 2.2ms preprocess, 19.4ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 26.1ms\n",
      "Speed: 4.2ms preprocess, 26.1ms inference, 2.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 20.3ms\n",
      "Speed: 2.7ms preprocess, 20.3ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 28.2ms\n",
      "Speed: 2.8ms preprocess, 28.2ms inference, 3.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 27.2ms\n",
      "Speed: 2.2ms preprocess, 27.2ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.7ms\n",
      "Speed: 1.8ms preprocess, 19.7ms inference, 2.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.3ms\n",
      "Speed: 2.4ms preprocess, 19.3ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.2ms\n",
      "Speed: 1.8ms preprocess, 19.2ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.1ms\n",
      "Speed: 1.9ms preprocess, 18.1ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.6ms\n",
      "Speed: 2.4ms preprocess, 17.6ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.5ms\n",
      "Speed: 2.2ms preprocess, 19.5ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.6ms\n",
      "Speed: 2.1ms preprocess, 17.6ms inference, 2.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.6ms\n",
      "Speed: 3.8ms preprocess, 18.6ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.8ms\n",
      "Speed: 2.9ms preprocess, 18.8ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.6ms\n",
      "Speed: 2.2ms preprocess, 19.6ms inference, 2.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.0ms\n",
      "Speed: 2.2ms preprocess, 19.0ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.5ms\n",
      "Speed: 2.0ms preprocess, 19.5ms inference, 2.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.5ms\n",
      "Speed: 3.0ms preprocess, 19.5ms inference, 2.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 27.6ms\n",
      "Speed: 2.3ms preprocess, 27.6ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.7ms\n",
      "Speed: 2.3ms preprocess, 19.7ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 27.4ms\n",
      "Speed: 2.4ms preprocess, 27.4ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 28.0ms\n",
      "Speed: 2.4ms preprocess, 28.0ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.6ms\n",
      "Speed: 2.3ms preprocess, 19.6ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 36.8ms\n",
      "Speed: 2.1ms preprocess, 36.8ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 36.7ms\n",
      "Speed: 2.7ms preprocess, 36.7ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.1ms\n",
      "Speed: 1.8ms preprocess, 18.1ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.2ms\n",
      "Speed: 2.3ms preprocess, 18.2ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.8ms\n",
      "Speed: 2.6ms preprocess, 17.8ms inference, 2.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.5ms\n",
      "Speed: 2.1ms preprocess, 17.5ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.7ms\n",
      "Speed: 2.0ms preprocess, 17.7ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.0ms\n",
      "Speed: 2.3ms preprocess, 17.0ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.8ms\n",
      "Speed: 2.5ms preprocess, 17.8ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.6ms\n",
      "Speed: 2.2ms preprocess, 17.6ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 29.2ms\n",
      "Speed: 2.4ms preprocess, 29.2ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.7ms\n",
      "Speed: 2.0ms preprocess, 17.7ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.4ms\n",
      "Speed: 1.8ms preprocess, 17.4ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.4ms\n",
      "Speed: 2.0ms preprocess, 17.4ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.0ms\n",
      "Speed: 2.5ms preprocess, 19.0ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 25.7ms\n",
      "Speed: 2.5ms preprocess, 25.7ms inference, 2.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.3ms\n",
      "Speed: 1.9ms preprocess, 18.3ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.5ms\n",
      "Speed: 2.2ms preprocess, 18.5ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.5ms\n",
      "Speed: 2.4ms preprocess, 19.5ms inference, 2.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.4ms\n",
      "Speed: 2.1ms preprocess, 19.4ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.8ms\n",
      "Speed: 1.8ms preprocess, 18.8ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.6ms\n",
      "Speed: 1.8ms preprocess, 18.6ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.7ms\n",
      "Speed: 1.8ms preprocess, 18.7ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.6ms\n",
      "Speed: 2.0ms preprocess, 19.6ms inference, 2.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.0ms\n",
      "Speed: 1.7ms preprocess, 19.0ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.9ms\n",
      "Speed: 1.6ms preprocess, 18.9ms inference, 2.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.9ms\n",
      "Speed: 2.1ms preprocess, 18.9ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.9ms\n",
      "Speed: 2.8ms preprocess, 18.9ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.6ms\n",
      "Speed: 1.6ms preprocess, 18.6ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 20.0ms\n",
      "Speed: 2.3ms preprocess, 20.0ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 20.3ms\n",
      "Speed: 1.7ms preprocess, 20.3ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 20.0ms\n",
      "Speed: 1.7ms preprocess, 20.0ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.8ms\n",
      "Speed: 2.2ms preprocess, 19.8ms inference, 2.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.1ms\n",
      "Speed: 2.3ms preprocess, 18.1ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 25.2ms\n",
      "Speed: 1.8ms preprocess, 25.2ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.0ms\n",
      "Speed: 2.9ms preprocess, 19.0ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.1ms\n",
      "Speed: 4.1ms preprocess, 19.1ms inference, 2.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.6ms\n",
      "Speed: 3.2ms preprocess, 18.6ms inference, 2.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.4ms\n",
      "Speed: 2.1ms preprocess, 18.4ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.1ms\n",
      "Speed: 2.9ms preprocess, 19.1ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.3ms\n",
      "Speed: 1.9ms preprocess, 19.3ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 20.4ms\n",
      "Speed: 2.4ms preprocess, 20.4ms inference, 2.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.5ms\n",
      "Speed: 2.1ms preprocess, 19.5ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.2ms\n",
      "Speed: 2.0ms preprocess, 19.2ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 20.6ms\n",
      "Speed: 1.8ms preprocess, 20.6ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.1ms\n",
      "Speed: 2.2ms preprocess, 19.1ms inference, 2.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 32.3ms\n",
      "Speed: 3.7ms preprocess, 32.3ms inference, 3.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.1ms\n",
      "Speed: 1.8ms preprocess, 19.1ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.9ms\n",
      "Speed: 1.9ms preprocess, 18.9ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 23.3ms\n",
      "Speed: 2.2ms preprocess, 23.3ms inference, 2.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 42.1ms\n",
      "Speed: 2.1ms preprocess, 42.1ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 24.6ms\n",
      "Speed: 1.8ms preprocess, 24.6ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.8ms\n",
      "Speed: 2.3ms preprocess, 18.8ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.5ms\n",
      "Speed: 2.0ms preprocess, 17.5ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.0ms\n",
      "Speed: 1.7ms preprocess, 16.0ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.8ms\n",
      "Speed: 1.9ms preprocess, 17.8ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 30.1ms\n",
      "Speed: 2.6ms preprocess, 30.1ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 23.2ms\n",
      "Speed: 1.8ms preprocess, 23.2ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.2ms\n",
      "Speed: 2.0ms preprocess, 16.2ms inference, 2.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.5ms\n",
      "Speed: 2.3ms preprocess, 17.5ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.7ms\n",
      "Speed: 2.0ms preprocess, 17.7ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.3ms\n",
      "Speed: 1.9ms preprocess, 15.3ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.1ms\n",
      "Speed: 1.9ms preprocess, 16.1ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.0ms\n",
      "Speed: 1.8ms preprocess, 16.0ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.0ms\n",
      "Speed: 2.4ms preprocess, 17.0ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.3ms\n",
      "Speed: 2.0ms preprocess, 15.3ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.5ms\n",
      "Speed: 2.4ms preprocess, 18.5ms inference, 2.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.5ms\n",
      "Speed: 2.3ms preprocess, 17.5ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.8ms\n",
      "Speed: 2.2ms preprocess, 18.8ms inference, 2.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.0ms\n",
      "Speed: 2.3ms preprocess, 18.0ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.3ms\n",
      "Speed: 1.5ms preprocess, 16.3ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.4ms\n",
      "Speed: 2.0ms preprocess, 12.4ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.3ms\n",
      "Speed: 1.8ms preprocess, 11.3ms inference, 2.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 14.1ms\n",
      "Speed: 1.8ms preprocess, 14.1ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.4ms\n",
      "Speed: 2.2ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.7ms\n",
      "Speed: 1.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.3ms\n",
      "Speed: 1.9ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.4ms\n",
      "Speed: 2.0ms preprocess, 11.4ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.3ms\n",
      "Speed: 1.8ms preprocess, 11.3ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.3ms\n",
      "Speed: 1.9ms preprocess, 11.3ms inference, 2.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.9ms\n",
      "Speed: 1.9ms preprocess, 18.9ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.0ms\n",
      "Speed: 1.6ms preprocess, 17.0ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.2ms\n",
      "Speed: 2.2ms preprocess, 19.2ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.9ms\n",
      "Speed: 2.0ms preprocess, 16.9ms inference, 2.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.9ms\n",
      "Speed: 2.3ms preprocess, 15.9ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 14.9ms\n",
      "Speed: 2.4ms preprocess, 14.9ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.8ms\n",
      "Speed: 1.5ms preprocess, 12.8ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 14.5ms\n",
      "Speed: 1.9ms preprocess, 14.5ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.2ms\n",
      "Speed: 1.8ms preprocess, 12.2ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.5ms\n",
      "Speed: 1.9ms preprocess, 19.5ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.5ms\n",
      "Speed: 1.9ms preprocess, 12.5ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.9ms\n",
      "Speed: 1.6ms preprocess, 15.9ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.0ms\n",
      "Speed: 1.9ms preprocess, 13.0ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.7ms\n",
      "Speed: 1.9ms preprocess, 11.7ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.4ms\n",
      "Speed: 1.9ms preprocess, 11.4ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.5ms\n",
      "Speed: 2.1ms preprocess, 11.5ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.2ms\n",
      "Speed: 1.5ms preprocess, 11.2ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 20.3ms\n",
      "Speed: 2.1ms preprocess, 20.3ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 20.1ms\n",
      "Speed: 2.2ms preprocess, 20.1ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.9ms\n",
      "Speed: 1.7ms preprocess, 18.9ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.6ms\n",
      "Speed: 1.8ms preprocess, 17.6ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.5ms\n",
      "Speed: 1.5ms preprocess, 19.5ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.2ms\n",
      "Speed: 2.0ms preprocess, 17.2ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.0ms\n",
      "Speed: 1.8ms preprocess, 19.0ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.2ms\n",
      "Speed: 1.8ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.5ms\n",
      "Speed: 2.0ms preprocess, 17.5ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.8ms\n",
      "Speed: 2.5ms preprocess, 12.8ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.4ms\n",
      "Speed: 1.6ms preprocess, 12.4ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.5ms\n",
      "Speed: 1.7ms preprocess, 11.5ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.3ms\n",
      "Speed: 1.5ms preprocess, 12.3ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 14.7ms\n",
      "Speed: 1.4ms preprocess, 14.7ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.4ms\n",
      "Speed: 1.8ms preprocess, 13.4ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.4ms\n",
      "Speed: 2.1ms preprocess, 17.4ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.6ms\n",
      "Speed: 3.6ms preprocess, 15.6ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.8ms\n",
      "Speed: 2.0ms preprocess, 13.8ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.1ms\n",
      "Speed: 1.6ms preprocess, 15.1ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.3ms\n",
      "Speed: 1.8ms preprocess, 19.3ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.6ms\n",
      "Speed: 1.8ms preprocess, 17.6ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.7ms\n",
      "Speed: 1.8ms preprocess, 18.7ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.1ms\n",
      "Speed: 2.3ms preprocess, 19.1ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.6ms\n",
      "Speed: 1.6ms preprocess, 17.6ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.6ms\n",
      "Speed: 2.1ms preprocess, 16.6ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.0ms\n",
      "Speed: 1.9ms preprocess, 13.0ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.4ms\n",
      "Speed: 1.9ms preprocess, 12.4ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.3ms\n",
      "Speed: 1.8ms preprocess, 13.3ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.7ms\n",
      "Speed: 2.0ms preprocess, 15.7ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.1ms\n",
      "Speed: 1.5ms preprocess, 12.1ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 14.8ms\n",
      "Speed: 2.1ms preprocess, 14.8ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.6ms\n",
      "Speed: 1.5ms preprocess, 12.6ms inference, 2.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.7ms\n",
      "Speed: 1.4ms preprocess, 12.7ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.7ms\n",
      "Speed: 2.4ms preprocess, 12.7ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 14.3ms\n",
      "Speed: 1.9ms preprocess, 14.3ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.4ms\n",
      "Speed: 2.3ms preprocess, 15.4ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.9ms\n",
      "Speed: 1.9ms preprocess, 15.9ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.5ms\n",
      "Speed: 2.1ms preprocess, 18.5ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.0ms\n",
      "Speed: 1.5ms preprocess, 18.0ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.0ms\n",
      "Speed: 1.8ms preprocess, 18.0ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.3ms\n",
      "Speed: 1.8ms preprocess, 18.3ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.6ms\n",
      "Speed: 2.0ms preprocess, 19.6ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.3ms\n",
      "Speed: 2.3ms preprocess, 15.3ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 14.1ms\n",
      "Speed: 2.3ms preprocess, 14.1ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.2ms\n",
      "Speed: 1.5ms preprocess, 13.2ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 23.6ms\n",
      "Speed: 1.9ms preprocess, 23.6ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.0ms\n",
      "Speed: 1.7ms preprocess, 18.0ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.3ms\n",
      "Speed: 1.8ms preprocess, 16.3ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 14.6ms\n",
      "Speed: 1.4ms preprocess, 14.6ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.3ms\n",
      "Speed: 1.8ms preprocess, 12.3ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 14.2ms\n",
      "Speed: 1.9ms preprocess, 14.2ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 14.3ms\n",
      "Speed: 1.8ms preprocess, 14.3ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.3ms\n",
      "Speed: 1.6ms preprocess, 16.3ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.9ms\n",
      "Speed: 2.5ms preprocess, 13.9ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 14.2ms\n",
      "Speed: 2.1ms preprocess, 14.2ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.7ms\n",
      "Speed: 1.9ms preprocess, 18.7ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 27.6ms\n",
      "Speed: 2.0ms preprocess, 27.6ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.3ms\n",
      "Speed: 2.1ms preprocess, 19.3ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.8ms\n",
      "Speed: 2.2ms preprocess, 18.8ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.6ms\n",
      "Speed: 2.1ms preprocess, 18.6ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.0ms\n",
      "Speed: 2.6ms preprocess, 19.0ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.3ms\n",
      "Speed: 2.6ms preprocess, 18.3ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.5ms\n",
      "Speed: 2.0ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 14.9ms\n",
      "Speed: 2.0ms preprocess, 14.9ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.7ms\n",
      "Speed: 1.9ms preprocess, 19.7ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.4ms\n",
      "Speed: 2.0ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.0ms\n",
      "Speed: 2.4ms preprocess, 17.0ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.6ms\n",
      "Speed: 2.3ms preprocess, 18.6ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.6ms\n",
      "Speed: 1.9ms preprocess, 13.6ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.4ms\n",
      "Speed: 1.5ms preprocess, 17.4ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.7ms\n",
      "Speed: 1.7ms preprocess, 16.7ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.9ms\n",
      "Speed: 1.7ms preprocess, 12.9ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.6ms\n",
      "Speed: 1.6ms preprocess, 13.6ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.1ms\n",
      "Speed: 1.7ms preprocess, 16.1ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.8ms\n",
      "Speed: 1.7ms preprocess, 12.8ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.7ms\n",
      "Speed: 1.9ms preprocess, 16.7ms inference, 2.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 20.3ms\n",
      "Speed: 2.0ms preprocess, 20.3ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.6ms\n",
      "Speed: 1.9ms preprocess, 18.6ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.5ms\n",
      "Speed: 1.9ms preprocess, 19.5ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.3ms\n",
      "Speed: 2.4ms preprocess, 18.3ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.7ms\n",
      "Speed: 2.1ms preprocess, 17.7ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.2ms\n",
      "Speed: 2.6ms preprocess, 19.2ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.7ms\n",
      "Speed: 2.3ms preprocess, 19.7ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 20.0ms\n",
      "Speed: 2.5ms preprocess, 20.0ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.7ms\n",
      "Speed: 1.8ms preprocess, 18.7ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.0ms\n",
      "Speed: 2.4ms preprocess, 19.0ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 22.0ms\n",
      "Speed: 2.9ms preprocess, 22.0ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 20.7ms\n",
      "Speed: 2.9ms preprocess, 20.7ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 20.6ms\n",
      "Speed: 4.4ms preprocess, 20.6ms inference, 3.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.3ms\n",
      "Speed: 2.3ms preprocess, 19.3ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.3ms\n",
      "Speed: 2.2ms preprocess, 17.3ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.8ms\n",
      "Speed: 3.1ms preprocess, 18.8ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.2ms\n",
      "Speed: 2.9ms preprocess, 17.2ms inference, 2.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.5ms\n",
      "Speed: 2.1ms preprocess, 18.5ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 14.8ms\n",
      "Speed: 2.0ms preprocess, 14.8ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 14.2ms\n",
      "Speed: 1.6ms preprocess, 14.2ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.9ms\n",
      "Speed: 1.5ms preprocess, 13.9ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 22.7ms\n",
      "Speed: 2.0ms preprocess, 22.7ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 25.2ms\n",
      "Speed: 2.0ms preprocess, 25.2ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.8ms\n",
      "Speed: 1.8ms preprocess, 12.8ms inference, 2.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 21.4ms\n",
      "Speed: 1.6ms preprocess, 21.4ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.8ms\n",
      "Speed: 1.9ms preprocess, 17.8ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.5ms\n",
      "Speed: 2.3ms preprocess, 17.5ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.1ms\n",
      "Speed: 1.7ms preprocess, 12.1ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.7ms\n",
      "Speed: 2.3ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.3ms\n",
      "Speed: 2.0ms preprocess, 17.3ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.2ms\n",
      "Speed: 2.2ms preprocess, 17.2ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.9ms\n",
      "Speed: 2.2ms preprocess, 16.9ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.4ms\n",
      "Speed: 2.0ms preprocess, 17.4ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.1ms\n",
      "Speed: 2.4ms preprocess, 19.1ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.1ms\n",
      "Speed: 1.6ms preprocess, 17.1ms inference, 2.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.1ms\n",
      "Speed: 2.1ms preprocess, 18.1ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.0ms\n",
      "Speed: 1.7ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 14.5ms\n",
      "Speed: 2.3ms preprocess, 14.5ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.5ms\n",
      "Speed: 2.0ms preprocess, 13.5ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.1ms\n",
      "Speed: 1.9ms preprocess, 17.1ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.7ms\n",
      "Speed: 1.9ms preprocess, 17.7ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.3ms\n",
      "Speed: 2.2ms preprocess, 13.3ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.0ms\n",
      "Speed: 1.8ms preprocess, 13.0ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.5ms\n",
      "Speed: 2.4ms preprocess, 13.5ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.4ms\n",
      "Speed: 1.9ms preprocess, 12.4ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 20.5ms\n",
      "Speed: 2.0ms preprocess, 20.5ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.8ms\n",
      "Speed: 1.8ms preprocess, 17.8ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.5ms\n",
      "Speed: 1.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 22.1ms\n",
      "Speed: 2.8ms preprocess, 22.1ms inference, 2.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.6ms\n",
      "Speed: 2.4ms preprocess, 19.6ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 20.1ms\n",
      "Speed: 2.3ms preprocess, 20.1ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.7ms\n",
      "Speed: 2.4ms preprocess, 16.7ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.9ms\n",
      "Speed: 2.1ms preprocess, 16.9ms inference, 2.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.7ms\n",
      "Speed: 2.2ms preprocess, 17.7ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.5ms\n",
      "Speed: 2.3ms preprocess, 17.5ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.7ms\n",
      "Speed: 1.7ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 14.6ms\n",
      "Speed: 2.3ms preprocess, 14.6ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.7ms\n",
      "Speed: 2.6ms preprocess, 17.7ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.1ms\n",
      "Speed: 1.6ms preprocess, 19.1ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 20.9ms\n",
      "Speed: 2.1ms preprocess, 20.9ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.1ms\n",
      "Speed: 1.9ms preprocess, 19.1ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.7ms\n",
      "Speed: 1.8ms preprocess, 13.7ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.7ms\n",
      "Speed: 1.8ms preprocess, 15.7ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.1ms\n",
      "Speed: 2.1ms preprocess, 15.1ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.4ms\n",
      "Speed: 2.0ms preprocess, 15.4ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.4ms\n",
      "Speed: 1.8ms preprocess, 13.4ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.4ms\n",
      "Speed: 1.7ms preprocess, 15.4ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.5ms\n",
      "Speed: 2.0ms preprocess, 19.5ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 18.0ms\n",
      "Speed: 1.7ms preprocess, 18.0ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.3ms\n",
      "Speed: 1.6ms preprocess, 19.3ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.3ms\n",
      "Speed: 1.8ms preprocess, 19.3ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.5ms\n",
      "Speed: 1.7ms preprocess, 17.5ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.6ms\n",
      "Speed: 1.7ms preprocess, 15.6ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.7ms\n",
      "Speed: 1.8ms preprocess, 12.7ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.2ms\n",
      "Speed: 2.6ms preprocess, 16.2ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.6ms\n",
      "Speed: 1.9ms preprocess, 15.6ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.1ms\n",
      "Speed: 1.6ms preprocess, 12.1ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.2ms\n",
      "Speed: 2.1ms preprocess, 12.2ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.9ms\n",
      "Speed: 2.2ms preprocess, 12.9ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.5ms\n",
      "Speed: 1.5ms preprocess, 12.5ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 14.2ms\n",
      "Speed: 1.6ms preprocess, 14.2ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.8ms\n",
      "Speed: 2.2ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.7ms\n",
      "Speed: 1.9ms preprocess, 16.7ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.6ms\n",
      "Speed: 2.1ms preprocess, 17.6ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.2ms\n",
      "Speed: 1.8ms preprocess, 19.2ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.6ms\n",
      "Speed: 2.2ms preprocess, 17.6ms inference, 2.4ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 17.3ms\n",
      "Speed: 3.0ms preprocess, 17.3ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.3ms\n",
      "Speed: 1.7ms preprocess, 19.3ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.6ms\n",
      "Speed: 1.9ms preprocess, 16.6ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.1ms\n",
      "Speed: 1.5ms preprocess, 13.1ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.5ms\n",
      "Speed: 1.5ms preprocess, 12.5ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 14.9ms\n",
      "Speed: 1.5ms preprocess, 14.9ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 1 gun, 17.2ms\n",
      "Speed: 1.6ms preprocess, 17.2ms inference, 3.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 1 gun, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 2.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.9ms\n",
      "Speed: 1.8ms preprocess, 13.9ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.7ms\n",
      "Speed: 1.4ms preprocess, 13.7ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 16.3ms\n",
      "Speed: 1.6ms preprocess, 16.3ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.9ms\n",
      "Speed: 2.2ms preprocess, 11.9ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 15.1ms\n",
      "Speed: 2.4ms preprocess, 15.1ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 11.7ms\n",
      "Speed: 2.1ms preprocess, 11.7ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 13.6ms\n",
      "Speed: 2.2ms preprocess, 13.6ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 12.2ms\n",
      "Speed: 1.8ms preprocess, 12.2ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 14.0ms\n",
      "Speed: 1.6ms preprocess, 14.0ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 224)\n",
      "\n",
      "0: 416x224 (no detections), 19.1ms\n",
      "Speed: 1.8ms preprocess, 19.1ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 224)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load the trained YOLOv8 model\n",
    "model = YOLO(r\"C:\\Users\\dsy92\\OneDrive\\Desktop\\Weapon-Detection\\model\\version2.pt\")\n",
    "\n",
    "# Path to your test video\n",
    "video_path = r\"C:\\Users\\dsy92\\OneDrive\\Desktop\\Weapon-Detection\\video\\vd8.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Read one frame to get shape\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Error: Could not read a frame.\")\n",
    "    exit()\n",
    "\n",
    "# Run a dummy prediction to get correct frame size\n",
    "results = model(frame)\n",
    "annotated_frame = results[0].plot()\n",
    "\n",
    "# Get the shape from the annotated frame\n",
    "height, width, _ = annotated_frame.shape\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Initialize the writer using actual annotated frame size\n",
    "out = cv2.VideoWriter(\"output_with_predictions.avi\", cv2.VideoWriter_fourcc(*'XVID'), fps, (width, height))\n",
    "\n",
    "# Reset video capture to beginning\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "# Allow window resizing\n",
    "cv2.namedWindow(\"YOLOv8 Weapon Detection\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"YOLOv8 Weapon Detection\", 1280, 720)\n",
    "\n",
    "# Process video\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    results = model(frame)\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    cv2.imshow(\"YOLOv8 Weapon Detection\", annotated_frame)\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3195c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
